{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🏔️ MODIS MOD10A1 Snow Albedo Analysis - Saskatchewan Glacier\n",
    "## Interactive Python/Geemap Implementation\n",
    "\n",
    "---\n",
    "\n",
    "### 📊 **Research-Grade Snow Albedo Analysis (2010-2024)**\n",
    "\n",
    "**Purpose**: Extract high-quality snow albedo data with comprehensive QA filtering using Python, geemap, and advanced statistical analysis.\n",
    "\n",
    "### 🎯 **Key Features**:\n",
    "- 📈 **Deep Statistical Analysis**: Sen's slope, Mann-Kendall trends, change points, anomaly detection\n",
    "- ⚡ **Interactive Widgets**: Real-time parameter optimization with ipywidgets\n",
    "- ☁️ **Comprehensive QA**: MOD10A1 v6.1 cloud detection and quality filtering\n",
    "- 🔬 **Research-Grade Filtering**: NDSI snow + glacier fraction + quality masks\n",
    "- 📊 **Enhanced Exports**: Annual summaries, daily time series, CSV generation\n",
    "- 🐍 **Python Ecosystem**: Integration with scipy, pandas, matplotlib, plotly\n",
    "\n",
    "### 🔍 **Quality Control Methods**:\n",
    "- 🌙 **Basic QA**: Excludes night, ocean, poor quality pixels\n",
    "- 🚩 **Algorithm Flags**: Comprehensive 8-bit QA filtering\n",
    "- 🗺️ **Spatial Filter**: Glacier fraction thresholds\n",
    "- ⏰ **Temporal Filter**: Statistical reliability minimums\n",
    "\n",
    "### 📅 **Analysis Period**: June-September (Extended melt season)\n",
    "### 🏔️ **Default Settings**: NDSI ≥0, Glacier ≥75%, Good+ QA quality\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📦 **Phase 1: Environment Setup & Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core geospatial and Earth Engine libraries\n",
    "import ee\n",
    "import geemap\n",
    "import geemap.colormaps as cm\n",
    "\n",
    "# Interactive widgets and visualization\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "from IPython.display import display, HTML, clear_output\n",
    "\n",
    "# Scientific computing and statistics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "from scipy import signal\n",
    "import pymannkendall as mk\n",
    "\n",
    "# Visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import seaborn as sns\n",
    "\n",
    "# Utilities\n",
    "import datetime\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "import json\n",
    "import os\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "\n",
    "# Configure matplotlib and seaborn\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"📦 Libraries imported successfully!\")\n",
    "print(f\"🌍 Earth Engine version: {ee.__version__}\")\n",
    "print(f\"🗺️ Geemap version: {geemap.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "source": "# Authenticate Google Earth Engine (run this first if needed)\n# Uncomment and run the line below if you need to authenticate:\n# ee.Authenticate()\n\n# Note: After authentication, restart the kernel and re-run all cells",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Authenticate and Initialize Earth Engine\ntry:\n    # Try to initialize first (if already authenticated)\n    ee.Initialize()\n    print(\"✅ Earth Engine initialized successfully!\")\nexcept Exception as e:\n    print(\"🔑 Earth Engine authentication required...\")\n    print(\"Please run the following in a separate code cell:\")\n    print(\"ee.Authenticate()\")\n    print(\"Then re-run this cell.\")\n    print(f\"Error: {e}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ⚙️ **Configuration Parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════════════════════════\n",
    "# CONFIGURATION PARAMETERS - Research-grade conservative settings\n",
    "# ═══════════════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "# Study period and temporal settings\n",
    "STUDY_YEARS = list(range(2010, 2025))  # 2010-2024\n",
    "SUMMER_START_MONTH = 6  # June (extended melt season)\n",
    "SUMMER_END_MONTH = 9    # September\n",
    "USE_PEAK_MELT_ONLY = False  # False = June-Sept, True = July-Sept\n",
    "\n",
    "# Filtering thresholds (conservative defaults)\n",
    "NDSI_SNOW_THRESHOLD = 0      # Minimum NDSI Snow Cover (index 0-100)\n",
    "GLACIER_FRACTION_THRESHOLD = 75  # Minimum glacier fraction (%) \n",
    "MIN_PIXEL_THRESHOLD = 10     # Minimum pixels for statistical reliability\n",
    "\n",
    "# Glacier fraction classification thresholds\n",
    "FRACTION_THRESHOLDS = [0.25, 0.50, 0.75, 0.90]\n",
    "\n",
    "# Class names for different glacier fraction categories\n",
    "FRACTION_CLASS_NAMES = [\n",
    "    'glacier_0_25pct', 'glacier_25_50pct', 'glacier_50_75pct', \n",
    "    'glacier_75_90pct', 'glacier_90_100pct'\n",
    "]\n",
    "\n",
    "ANNUAL_CLASS_NAMES = [\n",
    "    'glacier_0_25pct_high_snow', 'glacier_25_50pct_high_snow', \n",
    "    'glacier_50_75pct_high_snow', 'glacier_75_90pct_high_snow', \n",
    "    'glacier_90_100pct_high_snow'\n",
    "]\n",
    "\n",
    "# Quality Assessment configuration (conservative approach)\n",
    "QA_CONFIG = {\n",
    "    'basic_level': 'good',              # Good quality+ (0-1)\n",
    "    'exclude_inland_water': True,       # Exclude water/lakes\n",
    "    'exclude_visible_screen_fail': True,    # CRITICAL - corrupted data\n",
    "    'exclude_ndsi_screen_fail': True,       # CRITICAL - unreliable NDSI\n",
    "    'exclude_temp_height_fail': True,       # IMPORTANT - atypical conditions\n",
    "    'exclude_swir_anomaly': True,           # IMPORTANT - optical anomalies\n",
    "    'exclude_probably_cloudy': True,        # CRITICAL - v6.1 cloud detection\n",
    "    'exclude_probably_clear': False,        # Keep clear sky pixels\n",
    "    'exclude_high_solar_zenith': True       # IMPORTANT - poor lighting\n",
    "}\n",
    "\n",
    "# Asset paths\n",
    "GLACIER_ASSET_PATH = 'projects/tofunori/assets/Saskatchewan_glacier_2024_updated'\n",
    "\n",
    "print(\"⚙️ Configuration loaded:\")\n",
    "print(f\"   📅 Study period: {STUDY_YEARS[0]}-{STUDY_YEARS[-1]}\")\n",
    "print(f\"   🌞 Season: {'July-September' if USE_PEAK_MELT_ONLY else 'June-September'}\")\n",
    "print(f\"   ❄️ NDSI threshold: ≥{NDSI_SNOW_THRESHOLD}\")\n",
    "print(f\"   🏔️ Glacier fraction: ≥{GLACIER_FRACTION_THRESHOLD}%\")\n",
    "print(f\"   📊 Min pixels: ≥{MIN_PIXEL_THRESHOLD}\")\n",
    "print(f\"   ✅ QA level: {QA_CONFIG['basic_level']}+ quality\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🏔️ **Phase 2: Data Loading & Glacier Setup**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Load Saskatchewan glacier asset\nprint(\"🏔️ Loading Saskatchewan glacier asset...\")\n\ntry:\n    saskatchewan_glacier = ee.Image(GLACIER_ASSET_PATH)\n    glacier_mask = saskatchewan_glacier.gt(0)\n    \n    # Create glacier geometry for analysis - Fixed reduceToVectors call\n    glacier_geometry = glacier_mask.reduceToVectors(\n        reducer=ee.Reducer.count(),  # Added explicit reducer parameter\n        scale=30,\n        maxPixels=1e6,\n        tileScale=2\n    ).geometry()\n    \n    print(\"✅ Glacier asset loaded successfully!\")\n    \n    # Get glacier area information\n    glacier_area = glacier_mask.multiply(ee.Image.pixelArea()).reduceRegion(\n        reducer=ee.Reducer.sum(),\n        geometry=glacier_geometry,\n        scale=30,\n        maxPixels=1e9\n    )\n    \n    # Convert to client-side for display\n    area_info = glacier_area.getInfo()\n    glacier_area_km2 = area_info['constant'] / 1e6\n    \n    print(f\"📏 Glacier area: {glacier_area_km2:.2f} km²\")\n    \nexcept Exception as e:\n    print(f\"❌ Error loading glacier asset: {e}\")\n    print(\"Please check the asset path and permissions.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute static glacier fraction (optimization for repeated use)\n",
    "print(\"🔄 Computing static glacier fraction...\")\n",
    "\n",
    "try:\n",
    "    # Get MODIS projection reference\n",
    "    modis_reference = ee.ImageCollection('MODIS/061/MOD10A1') \\\n",
    "        .filterDate('2020-01-01', '2020-01-02') \\\n",
    "        .first()\n",
    "    \n",
    "    modis_projection = modis_reference.projection()\n",
    "    \n",
    "    # Create 30m raster and reproject to MODIS 500m\n",
    "    raster30 = ee.Image.constant(1) \\\n",
    "        .updateMask(glacier_mask) \\\n",
    "        .unmask(0) \\\n",
    "        .reproject(modis_projection, None, 30)\n",
    "    \n",
    "    # Compute glacier fraction at 500m resolution\n",
    "    STATIC_GLACIER_FRACTION = raster30.reduceResolution({\n",
    "        'reducer': ee.Reducer.mean(),\n",
    "        'maxPixels': 1024\n",
    "    }).reproject(modis_projection, None, 500)\n",
    "    \n",
    "    # Get fraction statistics\n",
    "    fraction_stats = STATIC_GLACIER_FRACTION.reduceRegion({\n",
    "        'reducer': ee.Reducer.minMax(),\n",
    "        'geometry': glacier_geometry,\n",
    "        'scale': 500,\n",
    "        'maxPixels': 1e9,\n",
    "        'tileScale': 2\n",
    "    })\n",
    "    \n",
    "    stats_info = fraction_stats.getInfo()\n",
    "    print(f\"✅ Glacier fraction computed successfully!\")\n",
    "    print(f\"📊 Fraction range: {stats_info['constant_min']:.3f} - {stats_info['constant_max']:.3f}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error computing glacier fraction: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🛠️ **Phase 3: Quality Assessment Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════════════════════════\n",
    "# QUALITY ASSESSMENT FUNCTIONS\n",
    "# Comprehensive QA filtering based on MOD10A1 v6.1 documentation\n",
    "# ═══════════════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "def get_basic_qa_mask(image: ee.Image, level: str = 'good') -> ee.Image:\n",
    "    \"\"\"\n",
    "    Create Basic QA mask based on quality level.\n",
    "    \n",
    "    Args:\n",
    "        image: MOD10A1 image with NDSI_Snow_Cover_Basic_QA band\n",
    "        level: Quality level ('best', 'good', 'ok', 'all')\n",
    "    \n",
    "    Returns:\n",
    "        Binary mask (1=keep, 0=exclude)\n",
    "    \"\"\"\n",
    "    basic_qa = image.select('NDSI_Snow_Cover_Basic_QA')\n",
    "    \n",
    "    # Quality thresholds based on MOD10A1 documentation\n",
    "    quality_thresholds = {\n",
    "        'best': 0,  # Best quality only\n",
    "        'good': 1,  # Good quality and above (0-1)\n",
    "        'ok': 2,    # OK quality and above (0-2)\n",
    "        'all': 3    # All quality levels (0-3)\n",
    "    }\n",
    "    \n",
    "    threshold = quality_thresholds.get(level, 1)\n",
    "    quality_mask = basic_qa.lte(threshold)\n",
    "    \n",
    "    # Always exclude night (211) and ocean (239)\n",
    "    exclude_mask = basic_qa.neq(211).And(basic_qa.neq(239))\n",
    "    \n",
    "    return quality_mask.And(exclude_mask)\n",
    "\n",
    "\n",
    "def get_algorithm_flags_mask(image: ee.Image, config: Dict) -> ee.Image:\n",
    "    \"\"\"\n",
    "    Create Algorithm Flags QA mask based on configuration.\n",
    "    \n",
    "    Args:\n",
    "        image: MOD10A1 image with NDSI_Snow_Cover_Algorithm_Flags_QA band\n",
    "        config: Dictionary with boolean flags for each bit\n",
    "    \n",
    "    Returns:\n",
    "        Binary mask (1=keep, 0=exclude)\n",
    "    \"\"\"\n",
    "    alg_flags = image.select('NDSI_Snow_Cover_Algorithm_Flags_QA')\n",
    "    mask = ee.Image(1)\n",
    "    \n",
    "    # QA bit mapping for MOD10A1 v6.1 Algorithm Flags\n",
    "    qa_bits = {\n",
    "        'exclude_inland_water': 0,        # Bit 0: Inland water\n",
    "        'exclude_visible_screen_fail': 1,  # Bit 1: Low visible screen failure\n",
    "        'exclude_ndsi_screen_fail': 2,     # Bit 2: Low NDSI screen failure\n",
    "        'exclude_temp_height_fail': 3,     # Bit 3: Temperature/height screen failure\n",
    "        'exclude_swir_anomaly': 4,         # Bit 4: SWIR reflectance anomaly\n",
    "        'exclude_probably_cloudy': 5,      # Bit 5: Probably cloudy (v6.1)\n",
    "        'exclude_probably_clear': 6,       # Bit 6: Probably clear (v6.1)\n",
    "        'exclude_high_solar_zenith': 7     # Bit 7: High solar zenith (>70°)\n",
    "    }\n",
    "    \n",
    "    # Apply each flag based on configuration\n",
    "    for flag_name, bit_position in qa_bits.items():\n",
    "        if config.get(flag_name, False):\n",
    "            bit_mask = ee.Number(2).pow(bit_position).int()\n",
    "            mask = mask.And(alg_flags.bitwiseAnd(bit_mask).eq(0))\n",
    "    \n",
    "    return mask\n",
    "\n",
    "\n",
    "def create_comprehensive_quality_mask(image: ee.Image, config: Dict) -> ee.Image:\n",
    "    \"\"\"\n",
    "    Create comprehensive quality mask combining Basic QA and Algorithm Flags.\n",
    "    \n",
    "    Args:\n",
    "        image: MOD10A1 image\n",
    "        config: QA configuration dictionary\n",
    "    \n",
    "    Returns:\n",
    "        Combined quality mask\n",
    "    \"\"\"\n",
    "    basic_mask = get_basic_qa_mask(image, config.get('basic_level', 'good'))\n",
    "    flags_mask = get_algorithm_flags_mask(image, config)\n",
    "    \n",
    "    return basic_mask.And(flags_mask)\n",
    "\n",
    "\n",
    "def create_fraction_masks(fraction_image: ee.Image, thresholds: List[float]) -> Dict[str, ee.Image]:\n",
    "    \"\"\"\n",
    "    Create glacier fraction classification masks.\n",
    "    \n",
    "    Args:\n",
    "        fraction_image: Glacier fraction image (0-1)\n",
    "        thresholds: List of fraction thresholds [0.25, 0.50, 0.75, 0.90]\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary of fraction masks\n",
    "    \"\"\"\n",
    "    masks = {}\n",
    "    masks['glacier_0_25pct'] = fraction_image.gt(0).And(fraction_image.lt(thresholds[0]))\n",
    "    masks['glacier_25_50pct'] = fraction_image.gte(thresholds[0]).And(fraction_image.lt(thresholds[1]))\n",
    "    masks['glacier_50_75pct'] = fraction_image.gte(thresholds[1]).And(fraction_image.lt(thresholds[2]))\n",
    "    masks['glacier_75_90pct'] = fraction_image.gte(thresholds[2]).And(fraction_image.lt(thresholds[3]))\n",
    "    masks['glacier_90_100pct'] = fraction_image.gte(thresholds[3])\n",
    "    \n",
    "    return masks\n",
    "\n",
    "\n",
    "print(\"🛠️ Quality assessment functions defined successfully!\")\n",
    "print(\"   ✅ Basic QA filtering\")\n",
    "print(\"   ✅ Algorithm Flags filtering (8-bit)\")\n",
    "print(\"   ✅ Comprehensive QA masking\")\n",
    "print(\"   ✅ Glacier fraction classification\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📊 **Phase 4: Data Processing Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════════════════════════\n",
    "# DATA PROCESSING FUNCTIONS\n",
    "# Core functions for annual and daily albedo analysis\n",
    "# ═══════════════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "def calculate_annual_albedo_statistics(year: int, \n",
    "                                     ndsi_threshold: int = NDSI_SNOW_THRESHOLD,\n",
    "                                     glacier_threshold: int = GLACIER_FRACTION_THRESHOLD,\n",
    "                                     min_pixels: int = MIN_PIXEL_THRESHOLD,\n",
    "                                     qa_config: Dict = QA_CONFIG) -> ee.Feature:\n",
    "    \"\"\"\n",
    "    Calculate annual albedo statistics for high snow cover areas.\n",
    "    \n",
    "    Args:\n",
    "        year: Analysis year\n",
    "        ndsi_threshold: Minimum NDSI snow cover threshold\n",
    "        glacier_threshold: Minimum glacier fraction percentage\n",
    "        min_pixels: Minimum pixels for statistical reliability\n",
    "        qa_config: Quality assessment configuration\n",
    "    \n",
    "    Returns:\n",
    "        Feature with annual statistics\n",
    "    \"\"\"\n",
    "    # Define date range\n",
    "    start_month = 7 if USE_PEAK_MELT_ONLY else SUMMER_START_MONTH\n",
    "    year_start = ee.Date.fromYMD(year, start_month, 1)\n",
    "    year_end = ee.Date.fromYMD(year, SUMMER_END_MONTH, 30)\n",
    "    \n",
    "    # Load and filter MOD10A1 collection\n",
    "    collection = ee.ImageCollection('MODIS/061/MOD10A1') \\\n",
    "        .filterDate(year_start, year_end) \\\n",
    "        .filterBounds(glacier_geometry) \\\n",
    "        .select(['NDSI_Snow_Cover', 'Snow_Albedo_Daily_Tile', \n",
    "                'NDSI_Snow_Cover_Basic_QA', 'NDSI_Snow_Cover_Algorithm_Flags_QA']) \\\n",
    "        .map(lambda img: img.clip(glacier_geometry))\n",
    "    \n",
    "    # Process each image\n",
    "    def process_image(img):\n",
    "        snow_cover = img.select('NDSI_Snow_Cover')\n",
    "        snow_albedo = img.select('Snow_Albedo_Daily_Tile')\n",
    "        \n",
    "        # Create quality masks\n",
    "        qa_mask = create_comprehensive_quality_mask(img, qa_config)\n",
    "        ndsi_mask = snow_cover.gte(ndsi_threshold)\n",
    "        glacier_mask = STATIC_GLACIER_FRACTION.gte(glacier_threshold / 100)\n",
    "        albedo_mask = snow_albedo.lte(100)\n",
    "        \n",
    "        # Combined mask\n",
    "        combined_mask = qa_mask.And(ndsi_mask).And(glacier_mask).And(albedo_mask)\n",
    "        \n",
    "        # Scale albedo to 0-1 range\n",
    "        albedo_scaled = snow_albedo.divide(100).updateMask(combined_mask).rename('albedo')\n",
    "        \n",
    "        # Create fraction masks and apply to albedo\n",
    "        fraction_masks = create_fraction_masks(STATIC_GLACIER_FRACTION, FRACTION_THRESHOLDS)\n",
    "        \n",
    "        masked_albedos = []\n",
    "        for i, class_name in enumerate(ANNUAL_CLASS_NAMES):\n",
    "            fraction_key = FRACTION_CLASS_NAMES[i]\n",
    "            masked_albedo = albedo_scaled.updateMask(fraction_masks[fraction_key]).rename(class_name)\n",
    "            masked_albedos.append(masked_albedo)\n",
    "        \n",
    "        # Add pixel count band\n",
    "        pixel_count = combined_mask.rename('high_snow_pixel_count')\n",
    "        \n",
    "        return ee.Image.cat(masked_albedos + [pixel_count])\n",
    "    \n",
    "    processed_collection = collection.map(process_image)\n",
    "    \n",
    "    # Calculate statistics\n",
    "    albedo_means = processed_collection.select(ANNUAL_CLASS_NAMES).mean()\n",
    "    pixel_count_total = processed_collection.select('high_snow_pixel_count').sum()\n",
    "    \n",
    "    # Reduce to get statistics\n",
    "    all_stats = albedo_means.reduceRegion({\n",
    "        'reducer': ee.Reducer.mean().combine(ee.Reducer.stdDev(), '', True).combine(ee.Reducer.count(), '', True),\n",
    "        'geometry': glacier_geometry,\n",
    "        'scale': 500,\n",
    "        'maxPixels': 1e9,\n",
    "        'tileScale': 4\n",
    "    })\n",
    "    \n",
    "    filtered_pixel_stats = pixel_count_total.reduceRegion({\n",
    "        'reducer': ee.Reducer.sum(),\n",
    "        'geometry': glacier_geometry,\n",
    "        'scale': 500,\n",
    "        'maxPixels': 1e9,\n",
    "        'tileScale': 4\n",
    "    })\n",
    "    \n",
    "    # Build properties\n",
    "    total_pixels = filtered_pixel_stats.get('high_snow_pixel_count')\n",
    "    sufficient_pixels = ee.Number(total_pixels).gte(min_pixels)\n",
    "    \n",
    "    properties = {\n",
    "        'year': year,\n",
    "        'ndsi_snow_threshold': ndsi_threshold,\n",
    "        'glacier_fraction_threshold': glacier_threshold,\n",
    "        'min_pixel_threshold': min_pixels,\n",
    "        'peak_melt_only': USE_PEAK_MELT_ONLY,\n",
    "        'total_filtered_pixels': total_pixels,\n",
    "        'sufficient_pixels': sufficient_pixels\n",
    "    }\n",
    "    \n",
    "    # Add class-specific statistics\n",
    "    for class_name in ANNUAL_CLASS_NAMES:\n",
    "        class_count = all_stats.get(class_name + '_count')\n",
    "        class_sufficient = ee.Number(class_count).gte(min_pixels)\n",
    "        \n",
    "        properties[class_name + '_mean'] = ee.Algorithms.If(class_sufficient, \n",
    "                                                          all_stats.get(class_name + '_mean'), None)\n",
    "        properties[class_name + '_stdDev'] = ee.Algorithms.If(class_sufficient, \n",
    "                                                            all_stats.get(class_name + '_stdDev'), None)\n",
    "        properties[class_name + '_count'] = class_count\n",
    "        properties[class_name + '_sufficient_pixels'] = class_sufficient\n",
    "    \n",
    "    return ee.Feature(None, properties)\n",
    "\n",
    "\n",
    "print(\"📊 Data processing functions defined successfully!\")\n",
    "print(\"   ✅ Annual albedo statistics calculation\")\n",
    "print(\"   ✅ Quality mask integration\")\n",
    "print(\"   ✅ Glacier fraction classification\")\n",
    "print(\"   ✅ Statistical reliability validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🗺️ **Phase 5: Interactive Map Setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create interactive map with geemap\n",
    "print(\"🗺️ Setting up interactive map...\")\n",
    "\n",
    "# Initialize map centered on Saskatchewan Glacier\n",
    "Map = geemap.Map(\n",
    "    center=[52.15, -117.28],  # Saskatchewan Glacier coordinates\n",
    "    zoom=12,\n",
    "    height='600px',\n",
    "    width='100%'\n",
    ")\n",
    "\n",
    "# Set satellite basemap for better glacier visualization\n",
    "Map.add_basemap('SATELLITE')\n",
    "\n",
    "# Add glacier mask to map\n",
    "glacier_vis = {\n",
    "    'palette': ['orange'],\n",
    "    'opacity': 0.7\n",
    "}\n",
    "\n",
    "Map.addLayer(glacier_mask.selfMask(), glacier_vis, 'Saskatchewan Glacier Mask')\n",
    "\n",
    "# Add glacier fraction layer (initially hidden)\n",
    "fraction_vis = {\n",
    "    'min': 0,\n",
    "    'max': 1,\n",
    "    'palette': ['red', 'orange', 'yellow', 'green', 'cyan', 'blue']\n",
    "}\n",
    "\n",
    "Map.addLayer(STATIC_GLACIER_FRACTION, fraction_vis, 'Glacier Fraction', False)\n",
    "\n",
    "print(\"✅ Interactive map setup complete!\")\n",
    "print(\"   🗺️ Centered on Saskatchewan Glacier\")\n",
    "print(\"   🛰️ Satellite basemap enabled\")\n",
    "print(\"   🏔️ Glacier mask and fraction layers added\")\n",
    "\n",
    "# Display the map\n",
    "Map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎛️ **Phase 6: Interactive Widgets & Controls**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════════════════════════\n",
    "# INTERACTIVE WIDGETS - Replace GEE UI with ipywidgets\n",
    "# ═══════════════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "# Global variables for current analysis\n",
    "current_image = None\n",
    "current_stats = {}\n",
    "current_qa_retention = {}\n",
    "\n",
    "# Create date picker widget\n",
    "date_picker = widgets.DatePicker(\n",
    "    description='Analysis Date:',\n",
    "    value=datetime(2023, 8, 7),\n",
    "    style={'description_width': 'initial'},\n",
    "    layout=widgets.Layout(width='300px')\n",
    ")\n",
    "\n",
    "# Parameter control sliders\n",
    "ndsi_slider = widgets.IntSlider(\n",
    "    value=NDSI_SNOW_THRESHOLD,\n",
    "    min=0,\n",
    "    max=100,\n",
    "    step=5,\n",
    "    description='NDSI Threshold:',\n",
    "    style={'description_width': 'initial'},\n",
    "    layout=widgets.Layout(width='400px')\n",
    ")\n",
    "\n",
    "glacier_fraction_slider = widgets.IntSlider(\n",
    "    value=GLACIER_FRACTION_THRESHOLD,\n",
    "    min=0,\n",
    "    max=100,\n",
    "    step=5,\n",
    "    description='Glacier Fraction (%):',\n",
    "    style={'description_width': 'initial'},\n",
    "    layout=widgets.Layout(width='400px')\n",
    ")\n",
    "\n",
    "min_pixel_slider = widgets.IntSlider(\n",
    "    value=MIN_PIXEL_THRESHOLD,\n",
    "    min=0,\n",
    "    max=100,\n",
    "    step=1,\n",
    "    description='Min Pixels:',\n",
    "    style={'description_width': 'initial'},\n",
    "    layout=widgets.Layout(width='400px')\n",
    ")\n",
    "\n",
    "# QA Level dropdown\n",
    "qa_level_dropdown = widgets.Dropdown(\n",
    "    options=[\n",
    "        ('Best quality only (0)', 'best'),\n",
    "        ('Good quality+ (0-1)', 'good'),\n",
    "        ('OK quality+ (0-2)', 'ok'),\n",
    "        ('All quality levels (0-3)', 'all')\n",
    "    ],\n",
    "    value='good',\n",
    "    description='Basic QA Level:',\n",
    "    style={'description_width': 'initial'},\n",
    "    layout=widgets.Layout(width='400px')\n",
    ")\n",
    "\n",
    "# Algorithm flags checkboxes\n",
    "flag_checkboxes = {\n",
    "    'exclude_inland_water': widgets.Checkbox(\n",
    "        value=QA_CONFIG['exclude_inland_water'],\n",
    "        description='Bit 0: Exclude Inland Water',\n",
    "        style={'description_width': 'initial'}\n",
    "    ),\n",
    "    'exclude_visible_screen_fail': widgets.Checkbox(\n",
    "        value=QA_CONFIG['exclude_visible_screen_fail'],\n",
    "        description='Bit 1: Exclude Visible Screen Fail',\n",
    "        style={'description_width': 'initial'}\n",
    "    ),\n",
    "    'exclude_ndsi_screen_fail': widgets.Checkbox(\n",
    "        value=QA_CONFIG['exclude_ndsi_screen_fail'],\n",
    "        description='Bit 2: Exclude NDSI Screen Fail',\n",
    "        style={'description_width': 'initial'}\n",
    "    ),\n",
    "    'exclude_temp_height_fail': widgets.Checkbox(\n",
    "        value=QA_CONFIG['exclude_temp_height_fail'],\n",
    "        description='Bit 3: Exclude Temp/Height Fail',\n",
    "        style={'description_width': 'initial'}\n",
    "    ),\n",
    "    'exclude_swir_anomaly': widgets.Checkbox(\n",
    "        value=QA_CONFIG['exclude_swir_anomaly'],\n",
    "        description='Bit 4: Exclude SWIR Anomaly',\n",
    "        style={'description_width': 'initial'}\n",
    "    ),\n",
    "    'exclude_probably_cloudy': widgets.Checkbox(\n",
    "        value=QA_CONFIG['exclude_probably_cloudy'],\n",
    "        description='Bit 5: Exclude Probably Cloudy (v6.1)',\n",
    "        style={'description_width': 'initial'}\n",
    "    ),\n",
    "    'exclude_probably_clear': widgets.Checkbox(\n",
    "        value=QA_CONFIG['exclude_probably_clear'],\n",
    "        description='Bit 6: Exclude Probably Clear (v6.1)',\n",
    "        style={'description_width': 'initial'}\n",
    "    ),\n",
    "    'exclude_high_solar_zenith': widgets.Checkbox(\n",
    "        value=QA_CONFIG['exclude_high_solar_zenith'],\n",
    "        description='Bit 7: Exclude High Solar Zenith',\n",
    "        style={'description_width': 'initial'}\n",
    "    )\n",
    "}\n",
    "\n",
    "# Action buttons\n",
    "load_data_button = widgets.Button(\n",
    "    description='🔄 Load Data',\n",
    "    button_style='primary',\n",
    "    layout=widgets.Layout(width='150px')\n",
    ")\n",
    "\n",
    "export_params_button = widgets.Button(\n",
    "    description='📤 Export Parameters',\n",
    "    button_style='success',\n",
    "    layout=widgets.Layout(width='200px')\n",
    ")\n",
    "\n",
    "# Output areas\n",
    "stats_output = widgets.Output()\n",
    "qa_output = widgets.Output()\n",
    "\n",
    "print(\\\"🎛️ Interactive widgets created successfully!\\\")\\nprint(\\\"   ✅ Date picker and parameter sliders\\\")\\nprint(\\\"   ✅ QA level dropdown and algorithm flags\\\")\\nprint(\\\"   ✅ Action buttons and output areas\\\")\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════════════════════════\n",
    "# WIDGET INTERACTION FUNCTIONS\n",
    "# ═══════════════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "def load_modis_data(selected_date):\n",
    "    \"\"\"\n",
    "    Load MODIS data for the selected date.\n",
    "    \"\"\"\n",
    "    global current_image\n",
    "    \n",
    "    try:\n",
    "        # Create date range (5-day window for data availability)\n",
    "        start_date = ee.Date(selected_date.strftime('%Y-%m-%d'))\n",
    "        end_date = start_date.advance(5, 'day')\n",
    "        \n",
    "        # Load MOD10A1 image\n",
    "        collection = ee.ImageCollection('MODIS/061/MOD10A1') \\\n",
    "            .filterDate(start_date, end_date) \\\n",
    "            .filterBounds(glacier_geometry) \\\n",
    "            .select(['NDSI_Snow_Cover', 'Snow_Albedo_Daily_Tile', \n",
    "                    'NDSI_Snow_Cover_Basic_QA', 'NDSI_Snow_Cover_Algorithm_Flags_QA'])\n",
    "        \n",
    "        current_image = collection.first().clip(glacier_geometry)\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\\\"❌ Error loading MODIS data: {e}\\\")\n",
    "        return False\n",
    "\n",
    "\n",
    "def update_filtering_and_visualization():\n",
    "    \\\"\\\"\\\"\n",
    "    Update filtering and map visualization based on current widget values.\n",
    "    \\\"\\\"\\\"\n",
    "    global current_stats, current_qa_retention\n",
    "    \n",
    "    if current_image is None:\n",
    "        with stats_output:\n",
    "            clear_output()\n",
    "            print(\\\"⚠️ Please load data first using the Load Data button.\\\")\n",
    "        return\n",
    "    \n",
    "    # Get current widget values\n",
    "    ndsi_threshold = ndsi_slider.value\n",
    "    glacier_threshold = glacier_fraction_slider.value\n",
    "    min_pixels = min_pixel_slider.value\n",
    "    qa_level = qa_level_dropdown.value\n",
    "    \n",
    "    # Build QA configuration from checkboxes\n",
    "    qa_config = {\n",
    "        'basic_level': qa_level,\n",
    "        **{key: checkbox.value for key, checkbox in flag_checkboxes.items()}\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Create quality and filtering masks\n",
    "        qa_mask = create_comprehensive_quality_mask(current_image, qa_config)\n",
    "        ndsi_mask = current_image.select('NDSI_Snow_Cover').gte(ndsi_threshold)\n",
    "        glacier_mask = STATIC_GLACIER_FRACTION.gte(glacier_threshold / 100)\n",
    "        albedo_mask = current_image.select('Snow_Albedo_Daily_Tile').lte(100)\n",
    "        \n",
    "        # Combined filtering mask\n",
    "        combined_mask = qa_mask.And(ndsi_mask).And(glacier_mask).And(albedo_mask)\n",
    "        \n",
    "        # Filtered albedo\n",
    "        filtered_albedo = current_image.select('Snow_Albedo_Daily_Tile') \\\n",
    "            .divide(100) \\\n",
    "            .updateMask(combined_mask) \\\n",
    "            .rename('filtered_albedo')\n",
    "        \n",
    "        # Calculate statistics\n",
    "        stats = filtered_albedo.reduceRegion({\n",
    "            'reducer': ee.Reducer.mean().combine(ee.Reducer.count(), '', True),\n",
    "            'geometry': glacier_geometry,\n",
    "            'scale': 500,\n",
    "            'maxPixels': 1e9,\n",
    "            'tileScale': 2\n",
    "        })\n",
    "        \n",
    "        # Calculate QA retention rate\n",
    "        total_glacier_pixels = glacier_mask.selfMask().reduceRegion({\n",
    "            'reducer': ee.Reducer.count(),\n",
    "            'geometry': glacier_geometry,\n",
    "            'scale': 500,\n",
    "            'maxPixels': 1e9\n",
    "        })\n",
    "        \n",
    "        retained_pixels = combined_mask.selfMask().reduceRegion({\n",
    "            'reducer': ee.Reducer.count(),\n",
    "            'geometry': glacier_geometry,\n",
    "            'scale': 500,\n",
    "            'maxPixels': 1e9\n",
    "        })\n",
    "        \n",
    "        # Get results\n",
    "        stats_info = stats.getInfo()\n",
    "        total_info = total_glacier_pixels.getInfo()\n",
    "        retained_info = retained_pixels.getInfo()\n",
    "        \n",
    "        current_stats = stats_info\n",
    "        current_qa_retention = {\n",
    "            'total': total_info.get('constant', 0),\n",
    "            'retained': retained_info.get('constant', 0)\n",
    "        }\n",
    "        \n",
    "        # Update map layers\n",
    "        update_map_layers(filtered_albedo, combined_mask)\n",
    "        \n",
    "        # Update statistics display\n",
    "        update_statistics_display()\n",
    "        \n",
    "    except Exception as e:\n",
    "        with stats_output:\n",
    "            clear_output()\n",
    "            print(f\\\"❌ Error in filtering update: {e}\\\")\n",
    "\n",
    "\n",
    "def update_map_layers(filtered_albedo, mask):\n",
    "    \\\"\\\"\\\"\n",
    "    Update map layers with current filtering results.\n",
    "    \\\"\\\"\\\"\n",
    "    try:\n",
    "        # Clear existing filtered layers (keep glacier mask)\n",
    "        layer_names = [layer.name for layer in Map.layers[2:]]  # Skip basemap and glacier mask\n",
    "        for name in layer_names:\n",
    "            if 'Filtered' in name or 'QA' in name or 'NDSI' in name:\n",
    "                Map.remove_layer_by_name(name)\n",
    "        \n",
    "        # Add filtered albedo layer\n",
    "        albedo_vis = {\n",
    "            'min': 0.4,\n",
    "            'max': 0.9,\n",
    "            'palette': ['red', 'orange', 'yellow', 'green', 'cyan', 'blue']\n",
    "        }\n",
    "        \n",
    "        Map.addLayer(filtered_albedo, albedo_vis, f'Filtered Albedo (NDSI≥{ndsi_slider.value}, G≥{glacier_fraction_slider.value}%)')\n",
    "        \n",
    "        # Add QA inspection layers (hidden by default)\n",
    "        Map.addLayer(current_image.select('NDSI_Snow_Cover'), \n",
    "                    {'min': 0, 'max': 100, 'palette': ['blue', 'white']}, \n",
    "                    'NDSI Snow Cover', False)\n",
    "        \n",
    "        Map.addLayer(current_image.select('NDSI_Snow_Cover_Basic_QA'), \n",
    "                    {'min': 0, 'max': 3, 'palette': ['green', 'yellow', 'orange', 'red']}, \n",
    "                    'Basic QA', False)\n",
    "        \n",
    "        Map.addLayer(current_image.select('NDSI_Snow_Cover_Algorithm_Flags_QA'), \n",
    "                    {'min': 0, 'max': 255, 'palette': ['black', 'red']}, \n",
    "                    'Algorithm Flags QA', False)\n",
    "                    \n",
    "    except Exception as e:\n",
    "        print(f\\\"⚠️ Map layer update warning: {e}\\\")\n",
    "\n",
    "\n",
    "def update_statistics_display():\n",
    "    \\\"\\\"\\\"\n",
    "    Update the statistics output display.\n",
    "    \\\"\\\"\\\"\n",
    "    with stats_output:\n",
    "        clear_output()\n",
    "        \n",
    "        if current_stats and 'filtered_albedo_mean' in current_stats:\n",
    "            mean_albedo = current_stats['filtered_albedo_mean']\n",
    "            pixel_count = current_stats.get('filtered_albedo_count', 0)\n",
    "            min_threshold = min_pixel_slider.value\n",
    "            \n",
    "            print(\\\"📊 Real-time Statistics (Optimized):\\\")\n",
    "            print(\\\"─\\\" * 40)\n",
    "            \n",
    "            if mean_albedo is not None and pixel_count > 0:\n",
    "                threshold_met = (min_threshold == 0 or pixel_count >= min_threshold)\n",
    "                \n",
    "                if threshold_met:\n",
    "                    print(f\\\"✅ Mean albedo: {mean_albedo:.4f}\\\")\n",
    "                    print(f\\\"📈 Qualified pixels: {pixel_count}\\\")\n",
    "                    print(f\\\"🎯 Filters: NDSI≥{ndsi_slider.value} AND Glacier≥{glacier_fraction_slider.value}%\\\")\n",
    "                    \n",
    "                    if min_threshold > 0:\n",
    "                        print(f\\\"📊 Pixel threshold (≥{min_threshold}): ✅ PASSED\\\")\n",
    "                else:\n",
    "                    print(f\\\"⚠️ Pixels found: {pixel_count}\\\")\n",
    "                    print(f\\\"📊 Required threshold: ≥{min_threshold} pixels\\\")\n",
    "                    print(f\\\"❌ Not enough qualified pixels\\\")\n",
    "            else:\n",
    "                print(f\\\"❌ No qualified pixels found\\\")\n",
    "                print(f\\\"💡 Try reducing thresholds\\\")\n",
    "        else:\n",
    "            print(\\\"⏳ No statistics available\\\")\n",
    "    \n",
    "    with qa_output:\n",
    "        clear_output()\n",
    "        \n",
    "        if current_qa_retention:\n",
    "            total = current_qa_retention.get('total', 0)\n",
    "            retained = current_qa_retention.get('retained', 0)\n",
    "            \n",
    "            if total > 0:\n",
    "                retention_rate = (retained / total) * 100\n",
    "                print(f\\\"🔍 QA Retention Rate:\\\")\n",
    "                print(f\\\"📊 {retained}/{total} pixels ({retention_rate:.1f}%)\\\")\n",
    "                print(f\\\"✅ QA Level: {qa_level_dropdown.value}+\\\")\n",
    "            else:\n",
    "                print(\\\"📊 QA retention: Calculating...\\\")\n",
    "\n",
    "\n",
    "print(\\\"🎛️ Widget interaction functions defined successfully!\\\")\n",
    "print(\\\"   ✅ Data loading and filtering updates\\\")\n",
    "print(\\\"   ✅ Map layer management\\\")\n",
    "print(\\\"   ✅ Real-time statistics display\\\")\n",
    "print(\\\"   ✅ QA retention calculations\\\")\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📈 **Phase 7: Enhanced Statistical Analysis with Python**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════════════════════════\n",
    "# ENHANCED STATISTICAL ANALYSIS - Using Python scientific libraries\n",
    "# Superior to JavaScript implementation with robust statistical methods\n",
    "# ═══════════════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "def analyze_temporal_trends(data_df: pd.DataFrame, value_col: str = 'glacier_90_100pct_high_snow_mean') -> Dict:\n",
    "    \\\"\\\"\\\"\n",
    "    Comprehensive temporal trend analysis using pyMannKendall and scipy.\n",
    "    \n",
    "    Args:\n",
    "        data_df: DataFrame with year and albedo data\n",
    "        value_col: Column name containing albedo values\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with comprehensive trend analysis results\n",
    "    \\\"\\\"\\\"\n",
    "    \n",
    "    # Filter valid data\n",
    "    valid_data = data_df.dropna(subset=[value_col])\n",
    "    \n",
    "    if len(valid_data) < 5:\n",
    "        return {'error': 'Insufficient data for trend analysis (need ≥5 years)'}\n",
    "    \n",
    "    years = valid_data['year'].values\n",
    "    values = valid_data[value_col].values\n",
    "    n_years = len(years)\n",
    "    \n",
    "    results = {\n",
    "        'dataset_info': {\n",
    "            'period': f\\\"{years[0]}-{years[-1]}\\\",\n",
    "            'n_years': n_years,\n",
    "            'mean_albedo': float(np.mean(values)),\n",
    "            'std_albedo': float(np.std(values))\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # 1. MANN-KENDALL TREND TEST (Superior to simple linear regression)\n",
    "    try:\n",
    "        mk_result = mk.original_test(values)\n",
    "        results['mann_kendall'] = {\n",
    "            'trend': mk_result.trend,\n",
    "            'p_value': float(mk_result.p),\n",
    "            'tau': float(mk_result.Tau),\n",
    "            'z_score': float(mk_result.z),\n",
    "            'slope': float(mk_result.slope) if hasattr(mk_result, 'slope') else None,\n",
    "            'interpretation': 'significant' if mk_result.p < 0.05 else 'not_significant'\n",
    "        }\n",
    "    except Exception as e:\n",
    "        results['mann_kendall'] = {'error': str(e)}\n",
    "    \n",
    "    # 2. SEN'S SLOPE ESTIMATOR (Robust trend magnitude)\n",
    "    try:\n",
    "        sens_result = mk.sens_slope(values)\n",
    "        results['sens_slope'] = {\n",
    "            'slope_per_year': float(sens_result.slope),\n",
    "            'slope_per_decade': float(sens_result.slope * 10),\n",
    "            'total_change': float(sens_result.slope * (years[-1] - years[0])),\n",
    "            'intercept': float(sens_result.intercept)\n",
    "        }\n",
    "    except Exception as e:\n",
    "        results['sens_slope'] = {'error': str(e)}\n",
    "    \n",
    "    # 3. AUTOCORRELATION ANALYSIS (Temporal persistence)\n",
    "    if n_years >= 3:\n",
    "        lag1_autocorr = np.corrcoef(values[:-1], values[1:])[0, 1]\n",
    "        results['autocorrelation'] = {\n",
    "            'lag1_correlation': float(lag1_autocorr),\n",
    "            'persistence_level': 'HIGH' if lag1_autocorr > 0.5 else 'MODERATE' if lag1_autocorr > 0.2 else 'LOW',\n",
    "            'interpretation': 'Strong year-to-year memory' if lag1_autocorr > 0.5 else \\n                           'Moderate temporal dependence' if lag1_autocorr > 0.2 else \\n                           'Weak temporal correlation'\\n        }\\n    \\n    # 4. CHANGE POINT DETECTION (Enhanced method)\\n    if n_years >= 10:\\n        # Simple change point detection using variance\\n        change_points = []\\n        threshold = 2 * np.std(values)  # 2-sigma threshold\\n        \\n        for i in range(3, n_years - 3):\\n            before_mean = np.mean(values[:i])\\n            after_mean = np.mean(values[i:])\\n            change_magnitude = abs(after_mean - before_mean)\\n            \\n            if change_magnitude > threshold:\\n                change_points.append({\\n                    'year': int(years[i]),\\n                    'magnitude': float(change_magnitude),\\n                    'direction': 'increase' if after_mean > before_mean else 'decrease'\\n                })\\n        \\n        results['change_points'] = change_points\\n    \\n    # 5. ANOMALY DETECTION (Z-score method)\\n    z_scores = (values - np.mean(values)) / np.std(values)\\n    anomalies = []\\n    \\n    for i, z_score in enumerate(z_scores):\\n        if abs(z_score) > 2.0:  # 2-sigma threshold\\n            anomalies.append({\\n                'year': int(years[i]),\\n                'albedo': float(values[i]),\\n                'z_score': float(z_score),\\n                'type': 'HIGH' if z_score > 0 else 'LOW'\\n            })\\n    \\n    results['anomalies'] = anomalies\\n    \\n    # 6. VARIABILITY ANALYSIS\\n    results['variability'] = {\\n        'coefficient_of_variation': float((np.std(values) / np.mean(values)) * 100),\\n        'range': float(np.max(values) - np.min(values)),\\n        'interquartile_range': float(np.percentile(values, 75) - np.percentile(values, 25))\\n    }\\n    \\n    # 7. CLIMATE SIGNAL ANALYSIS (Early vs Late period)\\n    if n_years >= 10:\\n        split_point = n_years // 2\\n        early_period = values[:split_point]\\n        late_period = values[-split_point:]\\n        \\n        early_mean = np.mean(early_period)\\n        late_mean = np.mean(late_period)\\n        period_diff = late_mean - early_mean\\n        relative_change = (period_diff / early_mean) * 100\\n        \\n        results['climate_signal'] = {\\n            'early_period': {\\n                'years': f\\\"{years[0]}-{years[split_point-1]}\\\",\\n                'mean': float(early_mean)\\n            },\\n            'late_period': {\\n                'years': f\\\"{years[-split_point]}-{years[-1]}\\\",\\n                'mean': float(late_mean)\\n            },\\n            'difference': float(period_diff),\\n            'relative_change_percent': float(relative_change),\\n            'signal_strength': 'STRONG' if abs(relative_change) > 5 else \\n                            'MODERATE' if abs(relative_change) > 2 else 'WEAK'\\n        }\\n    \\n    return results\\n\\n\\ndef create_comprehensive_visualization(data_df: pd.DataFrame, analysis_results: Dict, \\n                                     value_col: str = 'glacier_90_100pct_high_snow_mean') -> go.Figure:\\n    \\\"\\\"\\\"\n",
    "    Create comprehensive visualization with multiple subplots using Plotly.\n",
    "    \n",
    "    Args:\n",
    "        data_df: DataFrame with temporal data\n",
    "        analysis_results: Results from analyze_temporal_trends\n",
    "        value_col: Column name for albedo values\n",
    "    \n",
    "    Returns:\n",
    "        Plotly figure with multiple subplots\n",
    "    \\\"\\\"\\\"\n",
    "    \n",
    "    # Filter valid data\n",
    "    valid_data = data_df.dropna(subset=[value_col])\n",
    "    years = valid_data['year']\n",
    "    values = valid_data[value_col]\n",
    "    \n",
    "    # Create subplots\n",
    "    fig = make_subplots(\n",
    "        rows=2, cols=2,\n",
    "        subplot_titles=(\n",
    "            'Albedo Time Series with Trend',\n",
    "            'Anomaly Detection (Z-scores)',\n",
    "            'Statistical Distribution',\n",
    "            'Trend Components'\n",
    "        ),\n",
    "        specs=[[{\\\"secondary_y\\\": False}, {\\\"secondary_y\\\": False}],\n",
    "               [{\\\"secondary_y\\\": False}, {\\\"secondary_y\\\": False}]]\n",
    "    )\n",
    "    \n",
    "    # Plot 1: Time series with trend line\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=years, y=values, mode='lines+markers', \n",
    "                  name='Observed Albedo', line=dict(color='blue')),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # Add Sen's slope trend line if available\n",
    "    if 'sens_slope' in analysis_results and 'slope_per_year' in analysis_results['sens_slope']:\n",
    "        slope = analysis_results['sens_slope']['slope_per_year']\n",
    "        intercept = analysis_results['sens_slope']['intercept']\n",
    "        trend_line = slope * (years - years.iloc[0]) + values.iloc[0]\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Scatter(x=years, y=trend_line, mode='lines', \n",
    "                      name=\\\"Sen's Slope Trend\\\", line=dict(color='red', dash='dash')),\n",
    "            row=1, col=1\n",
    "        )\n",
    "    \n",
    "    # Plot 2: Anomaly detection\n",
    "    z_scores = (values - values.mean()) / values.std()\n",
    "    colors = ['red' if abs(z) > 2 else 'blue' for z in z_scores]\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=years, y=z_scores, mode='markers', \n",
    "                  marker=dict(color=colors, size=8),\n",
    "                  name='Z-scores'),\n",
    "        row=1, col=2\n",
    "    )\n",
    "    \n",
    "    # Add threshold lines\n",
    "    fig.add_hline(y=2, line_dash=\\\"dash\\\", line_color=\\\"red\\\", row=1, col=2)\n",
    "    fig.add_hline(y=-2, line_dash=\\\"dash\\\", line_color=\\\"red\\\", row=1, col=2)\n",
    "    \n",
    "    # Plot 3: Distribution\n",
    "    fig.add_trace(\n",
    "        go.Histogram(x=values, nbinsx=10, name='Distribution', \n",
    "                    marker_color='lightblue', opacity=0.7),\n",
    "        row=2, col=1\n",
    "    )\n",
    "    \n",
    "    # Plot 4: Trend components (if Mann-Kendall available)\n",
    "    if 'mann_kendall' in analysis_results and 'tau' in analysis_results['mann_kendall']:\n",
    "        components = ['Tau', 'P-value', 'Z-score']\n",
    "        mk_results = analysis_results['mann_kendall']\n",
    "        comp_values = [mk_results.get('tau', 0), mk_results.get('p_value', 0), mk_results.get('z_score', 0)]\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Bar(x=components, y=comp_values, name='Mann-Kendall Stats',\n",
    "                  marker_color=['green', 'orange', 'purple']),\n",
    "            row=2, col=2\n",
    "        )\n",
    "    \n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        title_text=\\\"Comprehensive Albedo Trend Analysis\\\",\n",
    "        height=800,\n",
    "        showlegend=True\n",
    "    )\n",
    "    \n",
    "    # Update axes labels\n",
    "    fig.update_xaxes(title_text=\\\"Year\\\", row=1, col=1)\n",
    "    fig.update_yaxes(title_text=\\\"Albedo\\\", row=1, col=1)\n",
    "    fig.update_xaxes(title_text=\\\"Year\\\", row=1, col=2)\n",
    "    fig.update_yaxes(title_text=\\\"Z-score\\\", row=1, col=2)\n",
    "    fig.update_xaxes(title_text=\\\"Albedo\\\", row=2, col=1)\n",
    "    fig.update_yaxes(title_text=\\\"Frequency\\\", row=2, col=1)\n",
    "    fig.update_xaxes(title_text=\\\"Component\\\", row=2, col=2)\n",
    "    fig.update_yaxes(title_text=\\\"Value\\\", row=2, col=2)\n",
    "    \n",
    "    return fig\n",
    "\n",
    "\n",
    "print(\\\"📈 Enhanced statistical analysis functions defined successfully!\\\")\n",
    "print(\\\"   🔬 Mann-Kendall trend test (non-parametric)\\\")\n",
    "print(\\\"   📐 Sen's slope estimator (robust trends)\\\")\n",
    "print(\\\"   🔄 Autocorrelation analysis (temporal persistence)\\\")\n",
    "print(\\\"   🎯 Change point detection (structural breaks)\\\")\n",
    "print(\\\"   ⚠️ Anomaly detection (z-score method)\\\")\n",
    "print(\\\"   📊 Comprehensive visualization (Plotly subplots)\\\")\n",
    "print(\\\"   🌡️ Climate signal analysis (early vs late period)\\\")\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gis-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}