{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ”ï¸ MODIS MOD10A1 Snow Albedo Analysis - Saskatchewan Glacier\n",
    "## Interactive Python/Geemap Implementation\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ“Š **Research-Grade Snow Albedo Analysis (2010-2024)**\n",
    "\n",
    "**Purpose**: Extract high-quality snow albedo data with comprehensive QA filtering using Python, geemap, and advanced statistical analysis.\n",
    "\n",
    "### ðŸŽ¯ **Key Features**:\n",
    "- ðŸ“ˆ **Deep Statistical Analysis**: Sen's slope, Mann-Kendall trends, change points, anomaly detection\n",
    "- âš¡ **Interactive Widgets**: Real-time parameter optimization with ipywidgets\n",
    "- â˜ï¸ **Comprehensive QA**: MOD10A1 v6.1 cloud detection and quality filtering\n",
    "- ðŸ”¬ **Research-Grade Filtering**: NDSI snow + glacier fraction + quality masks\n",
    "- ðŸ“Š **Enhanced Exports**: Annual summaries, daily time series, CSV generation\n",
    "- ðŸ **Python Ecosystem**: Integration with scipy, pandas, matplotlib, plotly\n",
    "\n",
    "### ðŸ” **Quality Control Methods**:\n",
    "- ðŸŒ™ **Basic QA**: Excludes night, ocean, poor quality pixels\n",
    "- ðŸš© **Algorithm Flags**: Comprehensive 8-bit QA filtering\n",
    "- ðŸ—ºï¸ **Spatial Filter**: Glacier fraction thresholds\n",
    "- â° **Temporal Filter**: Statistical reliability minimums\n",
    "\n",
    "### ðŸ“… **Analysis Period**: June-September (Extended melt season)\n",
    "### ðŸ”ï¸ **Default Settings**: NDSI â‰¥0, Glacier â‰¥75%, Good+ QA quality\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“¦ **Phase 1: Environment Setup & Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core geospatial and Earth Engine libraries\n",
    "import ee\n",
    "import geemap\n",
    "import geemap.colormaps as cm\n",
    "\n",
    "# Interactive widgets and visualization\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "from IPython.display import display, HTML, clear_output\n",
    "\n",
    "# Scientific computing and statistics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "from scipy import signal\n",
    "import pymannkendall as mk\n",
    "\n",
    "# Visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import seaborn as sns\n",
    "\n",
    "# Utilities\n",
    "import datetime\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "import json\n",
    "import os\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "\n",
    "# Configure matplotlib and seaborn\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"ðŸ“¦ Libraries imported successfully!\")\n",
    "print(f\"ðŸŒ Earth Engine version: {ee.__version__}\")\n",
    "print(f\"ðŸ—ºï¸ Geemap version: {geemap.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "source": "# Authenticate Google Earth Engine (run this first if needed)\n# Uncomment and run the line below if you need to authenticate:\n# ee.Authenticate()\n\n# Note: After authentication, restart the kernel and re-run all cells",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Authenticate and Initialize Earth Engine\ntry:\n    # Try to initialize first (if already authenticated)\n    ee.Initialize()\n    print(\"âœ… Earth Engine initialized successfully!\")\nexcept Exception as e:\n    print(\"ðŸ”‘ Earth Engine authentication required...\")\n    print(\"Please run the following in a separate code cell:\")\n    print(\"ee.Authenticate()\")\n    print(\"Then re-run this cell.\")\n    print(f\"Error: {e}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âš™ï¸ **Configuration Parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# CONFIGURATION PARAMETERS - Research-grade conservative settings\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "# Study period and temporal settings\n",
    "STUDY_YEARS = list(range(2010, 2025))  # 2010-2024\n",
    "SUMMER_START_MONTH = 6  # June (extended melt season)\n",
    "SUMMER_END_MONTH = 9    # September\n",
    "USE_PEAK_MELT_ONLY = False  # False = June-Sept, True = July-Sept\n",
    "\n",
    "# Filtering thresholds (conservative defaults)\n",
    "NDSI_SNOW_THRESHOLD = 0      # Minimum NDSI Snow Cover (index 0-100)\n",
    "GLACIER_FRACTION_THRESHOLD = 75  # Minimum glacier fraction (%) \n",
    "MIN_PIXEL_THRESHOLD = 10     # Minimum pixels for statistical reliability\n",
    "\n",
    "# Glacier fraction classification thresholds\n",
    "FRACTION_THRESHOLDS = [0.25, 0.50, 0.75, 0.90]\n",
    "\n",
    "# Class names for different glacier fraction categories\n",
    "FRACTION_CLASS_NAMES = [\n",
    "    'glacier_0_25pct', 'glacier_25_50pct', 'glacier_50_75pct', \n",
    "    'glacier_75_90pct', 'glacier_90_100pct'\n",
    "]\n",
    "\n",
    "ANNUAL_CLASS_NAMES = [\n",
    "    'glacier_0_25pct_high_snow', 'glacier_25_50pct_high_snow', \n",
    "    'glacier_50_75pct_high_snow', 'glacier_75_90pct_high_snow', \n",
    "    'glacier_90_100pct_high_snow'\n",
    "]\n",
    "\n",
    "# Quality Assessment configuration (conservative approach)\n",
    "QA_CONFIG = {\n",
    "    'basic_level': 'good',              # Good quality+ (0-1)\n",
    "    'exclude_inland_water': True,       # Exclude water/lakes\n",
    "    'exclude_visible_screen_fail': True,    # CRITICAL - corrupted data\n",
    "    'exclude_ndsi_screen_fail': True,       # CRITICAL - unreliable NDSI\n",
    "    'exclude_temp_height_fail': True,       # IMPORTANT - atypical conditions\n",
    "    'exclude_swir_anomaly': True,           # IMPORTANT - optical anomalies\n",
    "    'exclude_probably_cloudy': True,        # CRITICAL - v6.1 cloud detection\n",
    "    'exclude_probably_clear': False,        # Keep clear sky pixels\n",
    "    'exclude_high_solar_zenith': True       # IMPORTANT - poor lighting\n",
    "}\n",
    "\n",
    "# Asset paths\n",
    "GLACIER_ASSET_PATH = 'projects/tofunori/assets/Saskatchewan_glacier_2024_updated'\n",
    "\n",
    "print(\"âš™ï¸ Configuration loaded:\")\n",
    "print(f\"   ðŸ“… Study period: {STUDY_YEARS[0]}-{STUDY_YEARS[-1]}\")\n",
    "print(f\"   ðŸŒž Season: {'July-September' if USE_PEAK_MELT_ONLY else 'June-September'}\")\n",
    "print(f\"   â„ï¸ NDSI threshold: â‰¥{NDSI_SNOW_THRESHOLD}\")\n",
    "print(f\"   ðŸ”ï¸ Glacier fraction: â‰¥{GLACIER_FRACTION_THRESHOLD}%\")\n",
    "print(f\"   ðŸ“Š Min pixels: â‰¥{MIN_PIXEL_THRESHOLD}\")\n",
    "print(f\"   âœ… QA level: {QA_CONFIG['basic_level']}+ quality\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”ï¸ **Phase 2: Data Loading & Glacier Setup**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Load Saskatchewan glacier asset\nprint(\"ðŸ”ï¸ Loading Saskatchewan glacier asset...\")\n\ntry:\n    saskatchewan_glacier = ee.Image(GLACIER_ASSET_PATH)\n    glacier_mask = saskatchewan_glacier.gt(0)\n    \n    # Create glacier geometry for analysis - Fixed reduceToVectors call\n    glacier_geometry = glacier_mask.reduceToVectors(\n        reducer=ee.Reducer.count(),  # Added explicit reducer parameter\n        scale=30,\n        maxPixels=1e6,\n        tileScale=2\n    ).geometry()\n    \n    print(\"âœ… Glacier asset loaded successfully!\")\n    \n    # Get glacier area information\n    glacier_area = glacier_mask.multiply(ee.Image.pixelArea()).reduceRegion(\n        reducer=ee.Reducer.sum(),\n        geometry=glacier_geometry,\n        scale=30,\n        maxPixels=1e9\n    )\n    \n    # Convert to client-side for display\n    area_info = glacier_area.getInfo()\n    glacier_area_km2 = area_info['constant'] / 1e6\n    \n    print(f\"ðŸ“ Glacier area: {glacier_area_km2:.2f} kmÂ²\")\n    \nexcept Exception as e:\n    print(f\"âŒ Error loading glacier asset: {e}\")\n    print(\"Please check the asset path and permissions.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute static glacier fraction (optimization for repeated use)\n",
    "print(\"ðŸ”„ Computing static glacier fraction...\")\n",
    "\n",
    "try:\n",
    "    # Get MODIS projection reference\n",
    "    modis_reference = ee.ImageCollection('MODIS/061/MOD10A1') \\\n",
    "        .filterDate('2020-01-01', '2020-01-02') \\\n",
    "        .first()\n",
    "    \n",
    "    modis_projection = modis_reference.projection()\n",
    "    \n",
    "    # Create 30m raster and reproject to MODIS 500m\n",
    "    raster30 = ee.Image.constant(1) \\\n",
    "        .updateMask(glacier_mask) \\\n",
    "        .unmask(0) \\\n",
    "        .reproject(modis_projection, None, 30)\n",
    "    \n",
    "    # Compute glacier fraction at 500m resolution\n",
    "    STATIC_GLACIER_FRACTION = raster30.reduceResolution({\n",
    "        'reducer': ee.Reducer.mean(),\n",
    "        'maxPixels': 1024\n",
    "    }).reproject(modis_projection, None, 500)\n",
    "    \n",
    "    # Get fraction statistics\n",
    "    fraction_stats = STATIC_GLACIER_FRACTION.reduceRegion({\n",
    "        'reducer': ee.Reducer.minMax(),\n",
    "        'geometry': glacier_geometry,\n",
    "        'scale': 500,\n",
    "        'maxPixels': 1e9,\n",
    "        'tileScale': 2\n",
    "    })\n",
    "    \n",
    "    stats_info = fraction_stats.getInfo()\n",
    "    print(f\"âœ… Glacier fraction computed successfully!\")\n",
    "    print(f\"ðŸ“Š Fraction range: {stats_info['constant_min']:.3f} - {stats_info['constant_max']:.3f}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error computing glacier fraction: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ› ï¸ **Phase 3: Quality Assessment Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# QUALITY ASSESSMENT FUNCTIONS\n",
    "# Comprehensive QA filtering based on MOD10A1 v6.1 documentation\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "def get_basic_qa_mask(image: ee.Image, level: str = 'good') -> ee.Image:\n",
    "    \"\"\"\n",
    "    Create Basic QA mask based on quality level.\n",
    "    \n",
    "    Args:\n",
    "        image: MOD10A1 image with NDSI_Snow_Cover_Basic_QA band\n",
    "        level: Quality level ('best', 'good', 'ok', 'all')\n",
    "    \n",
    "    Returns:\n",
    "        Binary mask (1=keep, 0=exclude)\n",
    "    \"\"\"\n",
    "    basic_qa = image.select('NDSI_Snow_Cover_Basic_QA')\n",
    "    \n",
    "    # Quality thresholds based on MOD10A1 documentation\n",
    "    quality_thresholds = {\n",
    "        'best': 0,  # Best quality only\n",
    "        'good': 1,  # Good quality and above (0-1)\n",
    "        'ok': 2,    # OK quality and above (0-2)\n",
    "        'all': 3    # All quality levels (0-3)\n",
    "    }\n",
    "    \n",
    "    threshold = quality_thresholds.get(level, 1)\n",
    "    quality_mask = basic_qa.lte(threshold)\n",
    "    \n",
    "    # Always exclude night (211) and ocean (239)\n",
    "    exclude_mask = basic_qa.neq(211).And(basic_qa.neq(239))\n",
    "    \n",
    "    return quality_mask.And(exclude_mask)\n",
    "\n",
    "\n",
    "def get_algorithm_flags_mask(image: ee.Image, config: Dict) -> ee.Image:\n",
    "    \"\"\"\n",
    "    Create Algorithm Flags QA mask based on configuration.\n",
    "    \n",
    "    Args:\n",
    "        image: MOD10A1 image with NDSI_Snow_Cover_Algorithm_Flags_QA band\n",
    "        config: Dictionary with boolean flags for each bit\n",
    "    \n",
    "    Returns:\n",
    "        Binary mask (1=keep, 0=exclude)\n",
    "    \"\"\"\n",
    "    alg_flags = image.select('NDSI_Snow_Cover_Algorithm_Flags_QA')\n",
    "    mask = ee.Image(1)\n",
    "    \n",
    "    # QA bit mapping for MOD10A1 v6.1 Algorithm Flags\n",
    "    qa_bits = {\n",
    "        'exclude_inland_water': 0,        # Bit 0: Inland water\n",
    "        'exclude_visible_screen_fail': 1,  # Bit 1: Low visible screen failure\n",
    "        'exclude_ndsi_screen_fail': 2,     # Bit 2: Low NDSI screen failure\n",
    "        'exclude_temp_height_fail': 3,     # Bit 3: Temperature/height screen failure\n",
    "        'exclude_swir_anomaly': 4,         # Bit 4: SWIR reflectance anomaly\n",
    "        'exclude_probably_cloudy': 5,      # Bit 5: Probably cloudy (v6.1)\n",
    "        'exclude_probably_clear': 6,       # Bit 6: Probably clear (v6.1)\n",
    "        'exclude_high_solar_zenith': 7     # Bit 7: High solar zenith (>70Â°)\n",
    "    }\n",
    "    \n",
    "    # Apply each flag based on configuration\n",
    "    for flag_name, bit_position in qa_bits.items():\n",
    "        if config.get(flag_name, False):\n",
    "            bit_mask = ee.Number(2).pow(bit_position).int()\n",
    "            mask = mask.And(alg_flags.bitwiseAnd(bit_mask).eq(0))\n",
    "    \n",
    "    return mask\n",
    "\n",
    "\n",
    "def create_comprehensive_quality_mask(image: ee.Image, config: Dict) -> ee.Image:\n",
    "    \"\"\"\n",
    "    Create comprehensive quality mask combining Basic QA and Algorithm Flags.\n",
    "    \n",
    "    Args:\n",
    "        image: MOD10A1 image\n",
    "        config: QA configuration dictionary\n",
    "    \n",
    "    Returns:\n",
    "        Combined quality mask\n",
    "    \"\"\"\n",
    "    basic_mask = get_basic_qa_mask(image, config.get('basic_level', 'good'))\n",
    "    flags_mask = get_algorithm_flags_mask(image, config)\n",
    "    \n",
    "    return basic_mask.And(flags_mask)\n",
    "\n",
    "\n",
    "def create_fraction_masks(fraction_image: ee.Image, thresholds: List[float]) -> Dict[str, ee.Image]:\n",
    "    \"\"\"\n",
    "    Create glacier fraction classification masks.\n",
    "    \n",
    "    Args:\n",
    "        fraction_image: Glacier fraction image (0-1)\n",
    "        thresholds: List of fraction thresholds [0.25, 0.50, 0.75, 0.90]\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary of fraction masks\n",
    "    \"\"\"\n",
    "    masks = {}\n",
    "    masks['glacier_0_25pct'] = fraction_image.gt(0).And(fraction_image.lt(thresholds[0]))\n",
    "    masks['glacier_25_50pct'] = fraction_image.gte(thresholds[0]).And(fraction_image.lt(thresholds[1]))\n",
    "    masks['glacier_50_75pct'] = fraction_image.gte(thresholds[1]).And(fraction_image.lt(thresholds[2]))\n",
    "    masks['glacier_75_90pct'] = fraction_image.gte(thresholds[2]).And(fraction_image.lt(thresholds[3]))\n",
    "    masks['glacier_90_100pct'] = fraction_image.gte(thresholds[3])\n",
    "    \n",
    "    return masks\n",
    "\n",
    "\n",
    "print(\"ðŸ› ï¸ Quality assessment functions defined successfully!\")\n",
    "print(\"   âœ… Basic QA filtering\")\n",
    "print(\"   âœ… Algorithm Flags filtering (8-bit)\")\n",
    "print(\"   âœ… Comprehensive QA masking\")\n",
    "print(\"   âœ… Glacier fraction classification\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“Š **Phase 4: Data Processing Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# DATA PROCESSING FUNCTIONS\n",
    "# Core functions for annual and daily albedo analysis\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "def calculate_annual_albedo_statistics(year: int, \n",
    "                                     ndsi_threshold: int = NDSI_SNOW_THRESHOLD,\n",
    "                                     glacier_threshold: int = GLACIER_FRACTION_THRESHOLD,\n",
    "                                     min_pixels: int = MIN_PIXEL_THRESHOLD,\n",
    "                                     qa_config: Dict = QA_CONFIG) -> ee.Feature:\n",
    "    \"\"\"\n",
    "    Calculate annual albedo statistics for high snow cover areas.\n",
    "    \n",
    "    Args:\n",
    "        year: Analysis year\n",
    "        ndsi_threshold: Minimum NDSI snow cover threshold\n",
    "        glacier_threshold: Minimum glacier fraction percentage\n",
    "        min_pixels: Minimum pixels for statistical reliability\n",
    "        qa_config: Quality assessment configuration\n",
    "    \n",
    "    Returns:\n",
    "        Feature with annual statistics\n",
    "    \"\"\"\n",
    "    # Define date range\n",
    "    start_month = 7 if USE_PEAK_MELT_ONLY else SUMMER_START_MONTH\n",
    "    year_start = ee.Date.fromYMD(year, start_month, 1)\n",
    "    year_end = ee.Date.fromYMD(year, SUMMER_END_MONTH, 30)\n",
    "    \n",
    "    # Load and filter MOD10A1 collection\n",
    "    collection = ee.ImageCollection('MODIS/061/MOD10A1') \\\n",
    "        .filterDate(year_start, year_end) \\\n",
    "        .filterBounds(glacier_geometry) \\\n",
    "        .select(['NDSI_Snow_Cover', 'Snow_Albedo_Daily_Tile', \n",
    "                'NDSI_Snow_Cover_Basic_QA', 'NDSI_Snow_Cover_Algorithm_Flags_QA']) \\\n",
    "        .map(lambda img: img.clip(glacier_geometry))\n",
    "    \n",
    "    # Process each image\n",
    "    def process_image(img):\n",
    "        snow_cover = img.select('NDSI_Snow_Cover')\n",
    "        snow_albedo = img.select('Snow_Albedo_Daily_Tile')\n",
    "        \n",
    "        # Create quality masks\n",
    "        qa_mask = create_comprehensive_quality_mask(img, qa_config)\n",
    "        ndsi_mask = snow_cover.gte(ndsi_threshold)\n",
    "        glacier_mask = STATIC_GLACIER_FRACTION.gte(glacier_threshold / 100)\n",
    "        albedo_mask = snow_albedo.lte(100)\n",
    "        \n",
    "        # Combined mask\n",
    "        combined_mask = qa_mask.And(ndsi_mask).And(glacier_mask).And(albedo_mask)\n",
    "        \n",
    "        # Scale albedo to 0-1 range\n",
    "        albedo_scaled = snow_albedo.divide(100).updateMask(combined_mask).rename('albedo')\n",
    "        \n",
    "        # Create fraction masks and apply to albedo\n",
    "        fraction_masks = create_fraction_masks(STATIC_GLACIER_FRACTION, FRACTION_THRESHOLDS)\n",
    "        \n",
    "        masked_albedos = []\n",
    "        for i, class_name in enumerate(ANNUAL_CLASS_NAMES):\n",
    "            fraction_key = FRACTION_CLASS_NAMES[i]\n",
    "            masked_albedo = albedo_scaled.updateMask(fraction_masks[fraction_key]).rename(class_name)\n",
    "            masked_albedos.append(masked_albedo)\n",
    "        \n",
    "        # Add pixel count band\n",
    "        pixel_count = combined_mask.rename('high_snow_pixel_count')\n",
    "        \n",
    "        return ee.Image.cat(masked_albedos + [pixel_count])\n",
    "    \n",
    "    processed_collection = collection.map(process_image)\n",
    "    \n",
    "    # Calculate statistics\n",
    "    albedo_means = processed_collection.select(ANNUAL_CLASS_NAMES).mean()\n",
    "    pixel_count_total = processed_collection.select('high_snow_pixel_count').sum()\n",
    "    \n",
    "    # Reduce to get statistics\n",
    "    all_stats = albedo_means.reduceRegion({\n",
    "        'reducer': ee.Reducer.mean().combine(ee.Reducer.stdDev(), '', True).combine(ee.Reducer.count(), '', True),\n",
    "        'geometry': glacier_geometry,\n",
    "        'scale': 500,\n",
    "        'maxPixels': 1e9,\n",
    "        'tileScale': 4\n",
    "    })\n",
    "    \n",
    "    filtered_pixel_stats = pixel_count_total.reduceRegion({\n",
    "        'reducer': ee.Reducer.sum(),\n",
    "        'geometry': glacier_geometry,\n",
    "        'scale': 500,\n",
    "        'maxPixels': 1e9,\n",
    "        'tileScale': 4\n",
    "    })\n",
    "    \n",
    "    # Build properties\n",
    "    total_pixels = filtered_pixel_stats.get('high_snow_pixel_count')\n",
    "    sufficient_pixels = ee.Number(total_pixels).gte(min_pixels)\n",
    "    \n",
    "    properties = {\n",
    "        'year': year,\n",
    "        'ndsi_snow_threshold': ndsi_threshold,\n",
    "        'glacier_fraction_threshold': glacier_threshold,\n",
    "        'min_pixel_threshold': min_pixels,\n",
    "        'peak_melt_only': USE_PEAK_MELT_ONLY,\n",
    "        'total_filtered_pixels': total_pixels,\n",
    "        'sufficient_pixels': sufficient_pixels\n",
    "    }\n",
    "    \n",
    "    # Add class-specific statistics\n",
    "    for class_name in ANNUAL_CLASS_NAMES:\n",
    "        class_count = all_stats.get(class_name + '_count')\n",
    "        class_sufficient = ee.Number(class_count).gte(min_pixels)\n",
    "        \n",
    "        properties[class_name + '_mean'] = ee.Algorithms.If(class_sufficient, \n",
    "                                                          all_stats.get(class_name + '_mean'), None)\n",
    "        properties[class_name + '_stdDev'] = ee.Algorithms.If(class_sufficient, \n",
    "                                                            all_stats.get(class_name + '_stdDev'), None)\n",
    "        properties[class_name + '_count'] = class_count\n",
    "        properties[class_name + '_sufficient_pixels'] = class_sufficient\n",
    "    \n",
    "    return ee.Feature(None, properties)\n",
    "\n",
    "\n",
    "print(\"ðŸ“Š Data processing functions defined successfully!\")\n",
    "print(\"   âœ… Annual albedo statistics calculation\")\n",
    "print(\"   âœ… Quality mask integration\")\n",
    "print(\"   âœ… Glacier fraction classification\")\n",
    "print(\"   âœ… Statistical reliability validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ—ºï¸ **Phase 5: Interactive Map Setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create interactive map with geemap\n",
    "print(\"ðŸ—ºï¸ Setting up interactive map...\")\n",
    "\n",
    "# Initialize map centered on Saskatchewan Glacier\n",
    "Map = geemap.Map(\n",
    "    center=[52.15, -117.28],  # Saskatchewan Glacier coordinates\n",
    "    zoom=12,\n",
    "    height='600px',\n",
    "    width='100%'\n",
    ")\n",
    "\n",
    "# Set satellite basemap for better glacier visualization\n",
    "Map.add_basemap('SATELLITE')\n",
    "\n",
    "# Add glacier mask to map\n",
    "glacier_vis = {\n",
    "    'palette': ['orange'],\n",
    "    'opacity': 0.7\n",
    "}\n",
    "\n",
    "Map.addLayer(glacier_mask.selfMask(), glacier_vis, 'Saskatchewan Glacier Mask')\n",
    "\n",
    "# Add glacier fraction layer (initially hidden)\n",
    "fraction_vis = {\n",
    "    'min': 0,\n",
    "    'max': 1,\n",
    "    'palette': ['red', 'orange', 'yellow', 'green', 'cyan', 'blue']\n",
    "}\n",
    "\n",
    "Map.addLayer(STATIC_GLACIER_FRACTION, fraction_vis, 'Glacier Fraction', False)\n",
    "\n",
    "print(\"âœ… Interactive map setup complete!\")\n",
    "print(\"   ðŸ—ºï¸ Centered on Saskatchewan Glacier\")\n",
    "print(\"   ðŸ›°ï¸ Satellite basemap enabled\")\n",
    "print(\"   ðŸ”ï¸ Glacier mask and fraction layers added\")\n",
    "\n",
    "# Display the map\n",
    "Map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ›ï¸ **Phase 6: Interactive Widgets & Controls**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# INTERACTIVE WIDGETS - Replace GEE UI with ipywidgets\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "# Global variables for current analysis\n",
    "current_image = None\n",
    "current_stats = {}\n",
    "current_qa_retention = {}\n",
    "\n",
    "# Create date picker widget\n",
    "date_picker = widgets.DatePicker(\n",
    "    description='Analysis Date:',\n",
    "    value=datetime(2023, 8, 7),\n",
    "    style={'description_width': 'initial'},\n",
    "    layout=widgets.Layout(width='300px')\n",
    ")\n",
    "\n",
    "# Parameter control sliders\n",
    "ndsi_slider = widgets.IntSlider(\n",
    "    value=NDSI_SNOW_THRESHOLD,\n",
    "    min=0,\n",
    "    max=100,\n",
    "    step=5,\n",
    "    description='NDSI Threshold:',\n",
    "    style={'description_width': 'initial'},\n",
    "    layout=widgets.Layout(width='400px')\n",
    ")\n",
    "\n",
    "glacier_fraction_slider = widgets.IntSlider(\n",
    "    value=GLACIER_FRACTION_THRESHOLD,\n",
    "    min=0,\n",
    "    max=100,\n",
    "    step=5,\n",
    "    description='Glacier Fraction (%):',\n",
    "    style={'description_width': 'initial'},\n",
    "    layout=widgets.Layout(width='400px')\n",
    ")\n",
    "\n",
    "min_pixel_slider = widgets.IntSlider(\n",
    "    value=MIN_PIXEL_THRESHOLD,\n",
    "    min=0,\n",
    "    max=100,\n",
    "    step=1,\n",
    "    description='Min Pixels:',\n",
    "    style={'description_width': 'initial'},\n",
    "    layout=widgets.Layout(width='400px')\n",
    ")\n",
    "\n",
    "# QA Level dropdown\n",
    "qa_level_dropdown = widgets.Dropdown(\n",
    "    options=[\n",
    "        ('Best quality only (0)', 'best'),\n",
    "        ('Good quality+ (0-1)', 'good'),\n",
    "        ('OK quality+ (0-2)', 'ok'),\n",
    "        ('All quality levels (0-3)', 'all')\n",
    "    ],\n",
    "    value='good',\n",
    "    description='Basic QA Level:',\n",
    "    style={'description_width': 'initial'},\n",
    "    layout=widgets.Layout(width='400px')\n",
    ")\n",
    "\n",
    "# Algorithm flags checkboxes\n",
    "flag_checkboxes = {\n",
    "    'exclude_inland_water': widgets.Checkbox(\n",
    "        value=QA_CONFIG['exclude_inland_water'],\n",
    "        description='Bit 0: Exclude Inland Water',\n",
    "        style={'description_width': 'initial'}\n",
    "    ),\n",
    "    'exclude_visible_screen_fail': widgets.Checkbox(\n",
    "        value=QA_CONFIG['exclude_visible_screen_fail'],\n",
    "        description='Bit 1: Exclude Visible Screen Fail',\n",
    "        style={'description_width': 'initial'}\n",
    "    ),\n",
    "    'exclude_ndsi_screen_fail': widgets.Checkbox(\n",
    "        value=QA_CONFIG['exclude_ndsi_screen_fail'],\n",
    "        description='Bit 2: Exclude NDSI Screen Fail',\n",
    "        style={'description_width': 'initial'}\n",
    "    ),\n",
    "    'exclude_temp_height_fail': widgets.Checkbox(\n",
    "        value=QA_CONFIG['exclude_temp_height_fail'],\n",
    "        description='Bit 3: Exclude Temp/Height Fail',\n",
    "        style={'description_width': 'initial'}\n",
    "    ),\n",
    "    'exclude_swir_anomaly': widgets.Checkbox(\n",
    "        value=QA_CONFIG['exclude_swir_anomaly'],\n",
    "        description='Bit 4: Exclude SWIR Anomaly',\n",
    "        style={'description_width': 'initial'}\n",
    "    ),\n",
    "    'exclude_probably_cloudy': widgets.Checkbox(\n",
    "        value=QA_CONFIG['exclude_probably_cloudy'],\n",
    "        description='Bit 5: Exclude Probably Cloudy (v6.1)',\n",
    "        style={'description_width': 'initial'}\n",
    "    ),\n",
    "    'exclude_probably_clear': widgets.Checkbox(\n",
    "        value=QA_CONFIG['exclude_probably_clear'],\n",
    "        description='Bit 6: Exclude Probably Clear (v6.1)',\n",
    "        style={'description_width': 'initial'}\n",
    "    ),\n",
    "    'exclude_high_solar_zenith': widgets.Checkbox(\n",
    "        value=QA_CONFIG['exclude_high_solar_zenith'],\n",
    "        description='Bit 7: Exclude High Solar Zenith',\n",
    "        style={'description_width': 'initial'}\n",
    "    )\n",
    "}\n",
    "\n",
    "# Action buttons\n",
    "load_data_button = widgets.Button(\n",
    "    description='ðŸ”„ Load Data',\n",
    "    button_style='primary',\n",
    "    layout=widgets.Layout(width='150px')\n",
    ")\n",
    "\n",
    "export_params_button = widgets.Button(\n",
    "    description='ðŸ“¤ Export Parameters',\n",
    "    button_style='success',\n",
    "    layout=widgets.Layout(width='200px')\n",
    ")\n",
    "\n",
    "# Output areas\n",
    "stats_output = widgets.Output()\n",
    "qa_output = widgets.Output()\n",
    "\n",
    "print(\\\"ðŸŽ›ï¸ Interactive widgets created successfully!\\\")\\nprint(\\\"   âœ… Date picker and parameter sliders\\\")\\nprint(\\\"   âœ… QA level dropdown and algorithm flags\\\")\\nprint(\\\"   âœ… Action buttons and output areas\\\")\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# WIDGET INTERACTION FUNCTIONS\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "def load_modis_data(selected_date):\n",
    "    \"\"\"\n",
    "    Load MODIS data for the selected date.\n",
    "    \"\"\"\n",
    "    global current_image\n",
    "    \n",
    "    try:\n",
    "        # Create date range (5-day window for data availability)\n",
    "        start_date = ee.Date(selected_date.strftime('%Y-%m-%d'))\n",
    "        end_date = start_date.advance(5, 'day')\n",
    "        \n",
    "        # Load MOD10A1 image\n",
    "        collection = ee.ImageCollection('MODIS/061/MOD10A1') \\\n",
    "            .filterDate(start_date, end_date) \\\n",
    "            .filterBounds(glacier_geometry) \\\n",
    "            .select(['NDSI_Snow_Cover', 'Snow_Albedo_Daily_Tile', \n",
    "                    'NDSI_Snow_Cover_Basic_QA', 'NDSI_Snow_Cover_Algorithm_Flags_QA'])\n",
    "        \n",
    "        current_image = collection.first().clip(glacier_geometry)\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\\\"âŒ Error loading MODIS data: {e}\\\")\n",
    "        return False\n",
    "\n",
    "\n",
    "def update_filtering_and_visualization():\n",
    "    \\\"\\\"\\\"\n",
    "    Update filtering and map visualization based on current widget values.\n",
    "    \\\"\\\"\\\"\n",
    "    global current_stats, current_qa_retention\n",
    "    \n",
    "    if current_image is None:\n",
    "        with stats_output:\n",
    "            clear_output()\n",
    "            print(\\\"âš ï¸ Please load data first using the Load Data button.\\\")\n",
    "        return\n",
    "    \n",
    "    # Get current widget values\n",
    "    ndsi_threshold = ndsi_slider.value\n",
    "    glacier_threshold = glacier_fraction_slider.value\n",
    "    min_pixels = min_pixel_slider.value\n",
    "    qa_level = qa_level_dropdown.value\n",
    "    \n",
    "    # Build QA configuration from checkboxes\n",
    "    qa_config = {\n",
    "        'basic_level': qa_level,\n",
    "        **{key: checkbox.value for key, checkbox in flag_checkboxes.items()}\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Create quality and filtering masks\n",
    "        qa_mask = create_comprehensive_quality_mask(current_image, qa_config)\n",
    "        ndsi_mask = current_image.select('NDSI_Snow_Cover').gte(ndsi_threshold)\n",
    "        glacier_mask = STATIC_GLACIER_FRACTION.gte(glacier_threshold / 100)\n",
    "        albedo_mask = current_image.select('Snow_Albedo_Daily_Tile').lte(100)\n",
    "        \n",
    "        # Combined filtering mask\n",
    "        combined_mask = qa_mask.And(ndsi_mask).And(glacier_mask).And(albedo_mask)\n",
    "        \n",
    "        # Filtered albedo\n",
    "        filtered_albedo = current_image.select('Snow_Albedo_Daily_Tile') \\\n",
    "            .divide(100) \\\n",
    "            .updateMask(combined_mask) \\\n",
    "            .rename('filtered_albedo')\n",
    "        \n",
    "        # Calculate statistics\n",
    "        stats = filtered_albedo.reduceRegion({\n",
    "            'reducer': ee.Reducer.mean().combine(ee.Reducer.count(), '', True),\n",
    "            'geometry': glacier_geometry,\n",
    "            'scale': 500,\n",
    "            'maxPixels': 1e9,\n",
    "            'tileScale': 2\n",
    "        })\n",
    "        \n",
    "        # Calculate QA retention rate\n",
    "        total_glacier_pixels = glacier_mask.selfMask().reduceRegion({\n",
    "            'reducer': ee.Reducer.count(),\n",
    "            'geometry': glacier_geometry,\n",
    "            'scale': 500,\n",
    "            'maxPixels': 1e9\n",
    "        })\n",
    "        \n",
    "        retained_pixels = combined_mask.selfMask().reduceRegion({\n",
    "            'reducer': ee.Reducer.count(),\n",
    "            'geometry': glacier_geometry,\n",
    "            'scale': 500,\n",
    "            'maxPixels': 1e9\n",
    "        })\n",
    "        \n",
    "        # Get results\n",
    "        stats_info = stats.getInfo()\n",
    "        total_info = total_glacier_pixels.getInfo()\n",
    "        retained_info = retained_pixels.getInfo()\n",
    "        \n",
    "        current_stats = stats_info\n",
    "        current_qa_retention = {\n",
    "            'total': total_info.get('constant', 0),\n",
    "            'retained': retained_info.get('constant', 0)\n",
    "        }\n",
    "        \n",
    "        # Update map layers\n",
    "        update_map_layers(filtered_albedo, combined_mask)\n",
    "        \n",
    "        # Update statistics display\n",
    "        update_statistics_display()\n",
    "        \n",
    "    except Exception as e:\n",
    "        with stats_output:\n",
    "            clear_output()\n",
    "            print(f\\\"âŒ Error in filtering update: {e}\\\")\n",
    "\n",
    "\n",
    "def update_map_layers(filtered_albedo, mask):\n",
    "    \\\"\\\"\\\"\n",
    "    Update map layers with current filtering results.\n",
    "    \\\"\\\"\\\"\n",
    "    try:\n",
    "        # Clear existing filtered layers (keep glacier mask)\n",
    "        layer_names = [layer.name for layer in Map.layers[2:]]  # Skip basemap and glacier mask\n",
    "        for name in layer_names:\n",
    "            if 'Filtered' in name or 'QA' in name or 'NDSI' in name:\n",
    "                Map.remove_layer_by_name(name)\n",
    "        \n",
    "        # Add filtered albedo layer\n",
    "        albedo_vis = {\n",
    "            'min': 0.4,\n",
    "            'max': 0.9,\n",
    "            'palette': ['red', 'orange', 'yellow', 'green', 'cyan', 'blue']\n",
    "        }\n",
    "        \n",
    "        Map.addLayer(filtered_albedo, albedo_vis, f'Filtered Albedo (NDSIâ‰¥{ndsi_slider.value}, Gâ‰¥{glacier_fraction_slider.value}%)')\n",
    "        \n",
    "        # Add QA inspection layers (hidden by default)\n",
    "        Map.addLayer(current_image.select('NDSI_Snow_Cover'), \n",
    "                    {'min': 0, 'max': 100, 'palette': ['blue', 'white']}, \n",
    "                    'NDSI Snow Cover', False)\n",
    "        \n",
    "        Map.addLayer(current_image.select('NDSI_Snow_Cover_Basic_QA'), \n",
    "                    {'min': 0, 'max': 3, 'palette': ['green', 'yellow', 'orange', 'red']}, \n",
    "                    'Basic QA', False)\n",
    "        \n",
    "        Map.addLayer(current_image.select('NDSI_Snow_Cover_Algorithm_Flags_QA'), \n",
    "                    {'min': 0, 'max': 255, 'palette': ['black', 'red']}, \n",
    "                    'Algorithm Flags QA', False)\n",
    "                    \n",
    "    except Exception as e:\n",
    "        print(f\\\"âš ï¸ Map layer update warning: {e}\\\")\n",
    "\n",
    "\n",
    "def update_statistics_display():\n",
    "    \\\"\\\"\\\"\n",
    "    Update the statistics output display.\n",
    "    \\\"\\\"\\\"\n",
    "    with stats_output:\n",
    "        clear_output()\n",
    "        \n",
    "        if current_stats and 'filtered_albedo_mean' in current_stats:\n",
    "            mean_albedo = current_stats['filtered_albedo_mean']\n",
    "            pixel_count = current_stats.get('filtered_albedo_count', 0)\n",
    "            min_threshold = min_pixel_slider.value\n",
    "            \n",
    "            print(\\\"ðŸ“Š Real-time Statistics (Optimized):\\\")\n",
    "            print(\\\"â”€\\\" * 40)\n",
    "            \n",
    "            if mean_albedo is not None and pixel_count > 0:\n",
    "                threshold_met = (min_threshold == 0 or pixel_count >= min_threshold)\n",
    "                \n",
    "                if threshold_met:\n",
    "                    print(f\\\"âœ… Mean albedo: {mean_albedo:.4f}\\\")\n",
    "                    print(f\\\"ðŸ“ˆ Qualified pixels: {pixel_count}\\\")\n",
    "                    print(f\\\"ðŸŽ¯ Filters: NDSIâ‰¥{ndsi_slider.value} AND Glacierâ‰¥{glacier_fraction_slider.value}%\\\")\n",
    "                    \n",
    "                    if min_threshold > 0:\n",
    "                        print(f\\\"ðŸ“Š Pixel threshold (â‰¥{min_threshold}): âœ… PASSED\\\")\n",
    "                else:\n",
    "                    print(f\\\"âš ï¸ Pixels found: {pixel_count}\\\")\n",
    "                    print(f\\\"ðŸ“Š Required threshold: â‰¥{min_threshold} pixels\\\")\n",
    "                    print(f\\\"âŒ Not enough qualified pixels\\\")\n",
    "            else:\n",
    "                print(f\\\"âŒ No qualified pixels found\\\")\n",
    "                print(f\\\"ðŸ’¡ Try reducing thresholds\\\")\n",
    "        else:\n",
    "            print(\\\"â³ No statistics available\\\")\n",
    "    \n",
    "    with qa_output:\n",
    "        clear_output()\n",
    "        \n",
    "        if current_qa_retention:\n",
    "            total = current_qa_retention.get('total', 0)\n",
    "            retained = current_qa_retention.get('retained', 0)\n",
    "            \n",
    "            if total > 0:\n",
    "                retention_rate = (retained / total) * 100\n",
    "                print(f\\\"ðŸ” QA Retention Rate:\\\")\n",
    "                print(f\\\"ðŸ“Š {retained}/{total} pixels ({retention_rate:.1f}%)\\\")\n",
    "                print(f\\\"âœ… QA Level: {qa_level_dropdown.value}+\\\")\n",
    "            else:\n",
    "                print(\\\"ðŸ“Š QA retention: Calculating...\\\")\n",
    "\n",
    "\n",
    "print(\\\"ðŸŽ›ï¸ Widget interaction functions defined successfully!\\\")\n",
    "print(\\\"   âœ… Data loading and filtering updates\\\")\n",
    "print(\\\"   âœ… Map layer management\\\")\n",
    "print(\\\"   âœ… Real-time statistics display\\\")\n",
    "print(\\\"   âœ… QA retention calculations\\\")\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“ˆ **Phase 7: Enhanced Statistical Analysis with Python**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# ENHANCED STATISTICAL ANALYSIS - Using Python scientific libraries\n",
    "# Superior to JavaScript implementation with robust statistical methods\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "def analyze_temporal_trends(data_df: pd.DataFrame, value_col: str = 'glacier_90_100pct_high_snow_mean') -> Dict:\n",
    "    \\\"\\\"\\\"\n",
    "    Comprehensive temporal trend analysis using pyMannKendall and scipy.\n",
    "    \n",
    "    Args:\n",
    "        data_df: DataFrame with year and albedo data\n",
    "        value_col: Column name containing albedo values\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with comprehensive trend analysis results\n",
    "    \\\"\\\"\\\"\n",
    "    \n",
    "    # Filter valid data\n",
    "    valid_data = data_df.dropna(subset=[value_col])\n",
    "    \n",
    "    if len(valid_data) < 5:\n",
    "        return {'error': 'Insufficient data for trend analysis (need â‰¥5 years)'}\n",
    "    \n",
    "    years = valid_data['year'].values\n",
    "    values = valid_data[value_col].values\n",
    "    n_years = len(years)\n",
    "    \n",
    "    results = {\n",
    "        'dataset_info': {\n",
    "            'period': f\\\"{years[0]}-{years[-1]}\\\",\n",
    "            'n_years': n_years,\n",
    "            'mean_albedo': float(np.mean(values)),\n",
    "            'std_albedo': float(np.std(values))\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # 1. MANN-KENDALL TREND TEST (Superior to simple linear regression)\n",
    "    try:\n",
    "        mk_result = mk.original_test(values)\n",
    "        results['mann_kendall'] = {\n",
    "            'trend': mk_result.trend,\n",
    "            'p_value': float(mk_result.p),\n",
    "            'tau': float(mk_result.Tau),\n",
    "            'z_score': float(mk_result.z),\n",
    "            'slope': float(mk_result.slope) if hasattr(mk_result, 'slope') else None,\n",
    "            'interpretation': 'significant' if mk_result.p < 0.05 else 'not_significant'\n",
    "        }\n",
    "    except Exception as e:\n",
    "        results['mann_kendall'] = {'error': str(e)}\n",
    "    \n",
    "    # 2. SEN'S SLOPE ESTIMATOR (Robust trend magnitude)\n",
    "    try:\n",
    "        sens_result = mk.sens_slope(values)\n",
    "        results['sens_slope'] = {\n",
    "            'slope_per_year': float(sens_result.slope),\n",
    "            'slope_per_decade': float(sens_result.slope * 10),\n",
    "            'total_change': float(sens_result.slope * (years[-1] - years[0])),\n",
    "            'intercept': float(sens_result.intercept)\n",
    "        }\n",
    "    except Exception as e:\n",
    "        results['sens_slope'] = {'error': str(e)}\n",
    "    \n",
    "    # 3. AUTOCORRELATION ANALYSIS (Temporal persistence)\n",
    "    if n_years >= 3:\n",
    "        lag1_autocorr = np.corrcoef(values[:-1], values[1:])[0, 1]\n",
    "        results['autocorrelation'] = {\n",
    "            'lag1_correlation': float(lag1_autocorr),\n",
    "            'persistence_level': 'HIGH' if lag1_autocorr > 0.5 else 'MODERATE' if lag1_autocorr > 0.2 else 'LOW',\n",
    "            'interpretation': 'Strong year-to-year memory' if lag1_autocorr > 0.5 else \\n                           'Moderate temporal dependence' if lag1_autocorr > 0.2 else \\n                           'Weak temporal correlation'\\n        }\\n    \\n    # 4. CHANGE POINT DETECTION (Enhanced method)\\n    if n_years >= 10:\\n        # Simple change point detection using variance\\n        change_points = []\\n        threshold = 2 * np.std(values)  # 2-sigma threshold\\n        \\n        for i in range(3, n_years - 3):\\n            before_mean = np.mean(values[:i])\\n            after_mean = np.mean(values[i:])\\n            change_magnitude = abs(after_mean - before_mean)\\n            \\n            if change_magnitude > threshold:\\n                change_points.append({\\n                    'year': int(years[i]),\\n                    'magnitude': float(change_magnitude),\\n                    'direction': 'increase' if after_mean > before_mean else 'decrease'\\n                })\\n        \\n        results['change_points'] = change_points\\n    \\n    # 5. ANOMALY DETECTION (Z-score method)\\n    z_scores = (values - np.mean(values)) / np.std(values)\\n    anomalies = []\\n    \\n    for i, z_score in enumerate(z_scores):\\n        if abs(z_score) > 2.0:  # 2-sigma threshold\\n            anomalies.append({\\n                'year': int(years[i]),\\n                'albedo': float(values[i]),\\n                'z_score': float(z_score),\\n                'type': 'HIGH' if z_score > 0 else 'LOW'\\n            })\\n    \\n    results['anomalies'] = anomalies\\n    \\n    # 6. VARIABILITY ANALYSIS\\n    results['variability'] = {\\n        'coefficient_of_variation': float((np.std(values) / np.mean(values)) * 100),\\n        'range': float(np.max(values) - np.min(values)),\\n        'interquartile_range': float(np.percentile(values, 75) - np.percentile(values, 25))\\n    }\\n    \\n    # 7. CLIMATE SIGNAL ANALYSIS (Early vs Late period)\\n    if n_years >= 10:\\n        split_point = n_years // 2\\n        early_period = values[:split_point]\\n        late_period = values[-split_point:]\\n        \\n        early_mean = np.mean(early_period)\\n        late_mean = np.mean(late_period)\\n        period_diff = late_mean - early_mean\\n        relative_change = (period_diff / early_mean) * 100\\n        \\n        results['climate_signal'] = {\\n            'early_period': {\\n                'years': f\\\"{years[0]}-{years[split_point-1]}\\\",\\n                'mean': float(early_mean)\\n            },\\n            'late_period': {\\n                'years': f\\\"{years[-split_point]}-{years[-1]}\\\",\\n                'mean': float(late_mean)\\n            },\\n            'difference': float(period_diff),\\n            'relative_change_percent': float(relative_change),\\n            'signal_strength': 'STRONG' if abs(relative_change) > 5 else \\n                            'MODERATE' if abs(relative_change) > 2 else 'WEAK'\\n        }\\n    \\n    return results\\n\\n\\ndef create_comprehensive_visualization(data_df: pd.DataFrame, analysis_results: Dict, \\n                                     value_col: str = 'glacier_90_100pct_high_snow_mean') -> go.Figure:\\n    \\\"\\\"\\\"\n",
    "    Create comprehensive visualization with multiple subplots using Plotly.\n",
    "    \n",
    "    Args:\n",
    "        data_df: DataFrame with temporal data\n",
    "        analysis_results: Results from analyze_temporal_trends\n",
    "        value_col: Column name for albedo values\n",
    "    \n",
    "    Returns:\n",
    "        Plotly figure with multiple subplots\n",
    "    \\\"\\\"\\\"\n",
    "    \n",
    "    # Filter valid data\n",
    "    valid_data = data_df.dropna(subset=[value_col])\n",
    "    years = valid_data['year']\n",
    "    values = valid_data[value_col]\n",
    "    \n",
    "    # Create subplots\n",
    "    fig = make_subplots(\n",
    "        rows=2, cols=2,\n",
    "        subplot_titles=(\n",
    "            'Albedo Time Series with Trend',\n",
    "            'Anomaly Detection (Z-scores)',\n",
    "            'Statistical Distribution',\n",
    "            'Trend Components'\n",
    "        ),\n",
    "        specs=[[{\\\"secondary_y\\\": False}, {\\\"secondary_y\\\": False}],\n",
    "               [{\\\"secondary_y\\\": False}, {\\\"secondary_y\\\": False}]]\n",
    "    )\n",
    "    \n",
    "    # Plot 1: Time series with trend line\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=years, y=values, mode='lines+markers', \n",
    "                  name='Observed Albedo', line=dict(color='blue')),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # Add Sen's slope trend line if available\n",
    "    if 'sens_slope' in analysis_results and 'slope_per_year' in analysis_results['sens_slope']:\n",
    "        slope = analysis_results['sens_slope']['slope_per_year']\n",
    "        intercept = analysis_results['sens_slope']['intercept']\n",
    "        trend_line = slope * (years - years.iloc[0]) + values.iloc[0]\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Scatter(x=years, y=trend_line, mode='lines', \n",
    "                      name=\\\"Sen's Slope Trend\\\", line=dict(color='red', dash='dash')),\n",
    "            row=1, col=1\n",
    "        )\n",
    "    \n",
    "    # Plot 2: Anomaly detection\n",
    "    z_scores = (values - values.mean()) / values.std()\n",
    "    colors = ['red' if abs(z) > 2 else 'blue' for z in z_scores]\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=years, y=z_scores, mode='markers', \n",
    "                  marker=dict(color=colors, size=8),\n",
    "                  name='Z-scores'),\n",
    "        row=1, col=2\n",
    "    )\n",
    "    \n",
    "    # Add threshold lines\n",
    "    fig.add_hline(y=2, line_dash=\\\"dash\\\", line_color=\\\"red\\\", row=1, col=2)\n",
    "    fig.add_hline(y=-2, line_dash=\\\"dash\\\", line_color=\\\"red\\\", row=1, col=2)\n",
    "    \n",
    "    # Plot 3: Distribution\n",
    "    fig.add_trace(\n",
    "        go.Histogram(x=values, nbinsx=10, name='Distribution', \n",
    "                    marker_color='lightblue', opacity=0.7),\n",
    "        row=2, col=1\n",
    "    )\n",
    "    \n",
    "    # Plot 4: Trend components (if Mann-Kendall available)\n",
    "    if 'mann_kendall' in analysis_results and 'tau' in analysis_results['mann_kendall']:\n",
    "        components = ['Tau', 'P-value', 'Z-score']\n",
    "        mk_results = analysis_results['mann_kendall']\n",
    "        comp_values = [mk_results.get('tau', 0), mk_results.get('p_value', 0), mk_results.get('z_score', 0)]\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Bar(x=components, y=comp_values, name='Mann-Kendall Stats',\n",
    "                  marker_color=['green', 'orange', 'purple']),\n",
    "            row=2, col=2\n",
    "        )\n",
    "    \n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        title_text=\\\"Comprehensive Albedo Trend Analysis\\\",\n",
    "        height=800,\n",
    "        showlegend=True\n",
    "    )\n",
    "    \n",
    "    # Update axes labels\n",
    "    fig.update_xaxes(title_text=\\\"Year\\\", row=1, col=1)\n",
    "    fig.update_yaxes(title_text=\\\"Albedo\\\", row=1, col=1)\n",
    "    fig.update_xaxes(title_text=\\\"Year\\\", row=1, col=2)\n",
    "    fig.update_yaxes(title_text=\\\"Z-score\\\", row=1, col=2)\n",
    "    fig.update_xaxes(title_text=\\\"Albedo\\\", row=2, col=1)\n",
    "    fig.update_yaxes(title_text=\\\"Frequency\\\", row=2, col=1)\n",
    "    fig.update_xaxes(title_text=\\\"Component\\\", row=2, col=2)\n",
    "    fig.update_yaxes(title_text=\\\"Value\\\", row=2, col=2)\n",
    "    \n",
    "    return fig\n",
    "\n",
    "\n",
    "print(\\\"ðŸ“ˆ Enhanced statistical analysis functions defined successfully!\\\")\n",
    "print(\\\"   ðŸ”¬ Mann-Kendall trend test (non-parametric)\\\")\n",
    "print(\\\"   ðŸ“ Sen's slope estimator (robust trends)\\\")\n",
    "print(\\\"   ðŸ”„ Autocorrelation analysis (temporal persistence)\\\")\n",
    "print(\\\"   ðŸŽ¯ Change point detection (structural breaks)\\\")\n",
    "print(\\\"   âš ï¸ Anomaly detection (z-score method)\\\")\n",
    "print(\\\"   ðŸ“Š Comprehensive visualization (Plotly subplots)\\\")\n",
    "print(\\\"   ðŸŒ¡ï¸ Climate signal analysis (early vs late period)\\\")\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gis-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}